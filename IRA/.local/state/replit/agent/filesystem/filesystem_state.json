{"file_contents":{"app.py":{"content":"import streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport json\nimport os\nimport time\n\n# Import pipeline modules\nfrom pipeline.data_ingestion import DataIngestionEngine\nfrom pipeline.core_analysis import CoreAnalysisEngine\nfrom pipeline.ml_analysis import MLAnalysisEngine\nfrom pipeline.sentiment_analysis import SentimentAnalysisEngine\nfrom pipeline.report_generator import ReportGenerator\n\n# Configure Streamlit page\nst.set_page_config(\n    page_title=\"Portfolio Risk Analysis Pipeline\",\n    page_icon=\"ðŸ“Š\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\n# Initialize session state\nif 'pipeline_results' not in st.session_state:\n    st.session_state.pipeline_results = None\nif 'execution_time' not in st.session_state:\n    st.session_state.execution_time = None\n\ndef main():\n    st.title(\"ðŸ“Š Portfolio Risk Analysis Pipeline\")\n    st.markdown(\"---\")\n    \n    # Sidebar for pipeline controls\n    st.sidebar.header(\"Pipeline Controls\")\n    \n    # Portfolio selection\n    st.sidebar.subheader(\"Portfolio Configuration\")\n    portfolio_size = st.sidebar.slider(\"Portfolio Size\", min_value=10, max_value=100, value=25)\n    \n    # Pipeline execution button\n    execute_pipeline = st.sidebar.button(\"ðŸš€ Execute Full Pipeline\", type=\"primary\")\n    \n    # Stage-by-stage execution\n    st.sidebar.subheader(\"Stage-by-Stage Execution\")\n    stage1_btn = st.sidebar.button(\"Stage 1: Data Ingestion\")\n    stage2_btn = st.sidebar.button(\"Stage 2: Core Analysis\")\n    stage3_btn = st.sidebar.button(\"Stage 3: ML Analysis\")\n    stage4_btn = st.sidebar.button(\"Stage 4: Sentiment Analysis\")\n    stage5_btn = st.sidebar.button(\"Stage 5: Report Generation\")\n    \n    # Main content area\n    if execute_pipeline:\n        execute_full_pipeline(portfolio_size)\n    \n    # Individual stage execution\n    if stage1_btn:\n        execute_stage_1(portfolio_size)\n    elif stage2_btn:\n        execute_stage_2()\n    elif stage3_btn:\n        execute_stage_3()\n    elif stage4_btn:\n        execute_stage_4()\n    elif stage5_btn:\n        execute_stage_5()\n    \n    # Display results if available\n    if st.session_state.pipeline_results:\n        display_pipeline_results()\n\ndef execute_full_pipeline(portfolio_size):\n    \"\"\"Execute the complete five-stage pipeline\"\"\"\n    st.header(\"ðŸ”„ Executing Full Pipeline\")\n    \n    # Progress tracking\n    progress_bar = st.progress(0)\n    status_text = st.empty()\n    \n    start_time = time.time()\n    \n    try:\n        # Stage 1: Data Ingestion\n        status_text.text(\"Stage 1: Ingesting Bloomberg data...\")\n        progress_bar.progress(10)\n        \n        data_engine = DataIngestionEngine()\n        portfolio_data = data_engine.ingest_portfolio_data(portfolio_size)\n        \n        progress_bar.progress(25)\n        st.success(f\"âœ… Stage 1 Complete: Ingested data for {len(portfolio_data)} assets\")\n        \n        # Stage 2: Core Analysis\n        status_text.text(\"Stage 2: Running core risk analysis...\")\n        progress_bar.progress(40)\n        \n        analysis_engine = CoreAnalysisEngine()\n        analysis_results = analysis_engine.analyze_portfolio(portfolio_data)\n        \n        progress_bar.progress(50)\n        red_flags = len([a for a in analysis_results if a['risk_rating'] == 'RED'])\n        yellow_flags = len([a for a in analysis_results if a['risk_rating'] == 'YELLOW'])\n        st.success(f\"âœ… Stage 2 Complete: {red_flags} RED flags, {yellow_flags} YELLOW flags\")\n        \n        # Stage 3: ML Analysis\n        status_text.text(\"Stage 3: Running ML analysis (Anomaly Detection & Risk Prediction)...\")\n        progress_bar.progress(60)\n        \n        ml_engine = MLAnalysisEngine()\n        ml_results = ml_engine.analyze_portfolio_ml(analysis_results)\n        \n        progress_bar.progress(70)\n        anomaly_count = ml_results['ml_summary']['anomaly_summary']['total_anomalies']\n        st.success(f\"âœ… Stage 3 Complete: {anomaly_count} anomalies detected, ML model trained\")\n        \n        # Stage 4: Sentiment Analysis\n        status_text.text(\"Stage 4: Analyzing sentiment for flagged assets...\")\n        progress_bar.progress(80)\n        \n        sentiment_engine = SentimentAnalysisEngine()\n        red_flagged_assets = [a for a in analysis_results if a['risk_rating'] == 'RED']\n        sentiment_results = sentiment_engine.analyze_sentiment(red_flagged_assets)\n        \n        progress_bar.progress(90)\n        st.success(f\"âœ… Stage 4 Complete: Sentiment analysis for {len(sentiment_results)} RED-flagged assets\")\n        \n        # Stage 5: Report Generation\n        status_text.text(\"Stage 5: Generating PDF report with ML insights...\")\n        progress_bar.progress(95)\n        \n        report_generator = ReportGenerator()\n        report_files = report_generator.generate_report(\n            portfolio_data, analysis_results, sentiment_results, ml_results\n        )\n        \n        progress_bar.progress(100)\n        \n        end_time = time.time()\n        execution_time = end_time - start_time\n        \n        st.session_state.pipeline_results = {\n            'portfolio_data': portfolio_data,\n            'analysis_results': analysis_results,\n            'ml_results': ml_results,\n            'sentiment_results': sentiment_results,\n            'pdf_path': report_files['pdf_path'],\n            'portfolio_csv': report_files['portfolio_csv'],\n            'analysis_csv': report_files['analysis_csv'],\n            'red_flags': red_flags,\n            'yellow_flags': yellow_flags\n        }\n        st.session_state.execution_time = execution_time\n        \n        status_text.text(\"Pipeline execution completed!\")\n        st.success(f\"âœ… Pipeline Complete in {execution_time:.2f} seconds!\")\n        \n        # Offer file downloads\n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            if os.path.exists(report_files['pdf_path']):\n                with open(report_files['pdf_path'], 'rb') as pdf_file:\n                    st.download_button(\n                        label=\"ðŸ“„ Download PDF Report\",\n                        data=pdf_file.read(),\n                        file_name=os.path.basename(report_files['pdf_path']),\n                        mime=\"application/pdf\"\n                    )\n        \n        with col2:\n            if os.path.exists(report_files['portfolio_csv']):\n                with open(report_files['portfolio_csv'], 'rb') as csv_file:\n                    st.download_button(\n                        label=\"ðŸ“Š Download Portfolio CSV\",\n                        data=csv_file.read(),\n                        file_name=os.path.basename(report_files['portfolio_csv']),\n                        mime=\"text/csv\"\n                    )\n        \n        with col3:\n            if os.path.exists(report_files['analysis_csv']):\n                with open(report_files['analysis_csv'], 'rb') as csv_file:\n                    st.download_button(\n                        label=\"ðŸ“ˆ Download Risk Analysis CSV\",\n                        data=csv_file.read(),\n                        file_name=os.path.basename(report_files['analysis_csv']),\n                        mime=\"text/csv\"\n                    )\n        \n    except Exception as e:\n        st.error(f\"âŒ Pipeline execution failed: {str(e)}\")\n        status_text.text(\"Pipeline execution failed!\")\n        progress_bar.progress(0)\n\ndef execute_stage_1(portfolio_size):\n    \"\"\"Execute Stage 1: Data Ingestion\"\"\"\n    st.header(\"ðŸ“¥ Stage 1: Data Ingestion\")\n    \n    with st.spinner(\"Connecting to Bloomberg API and fetching data...\"):\n        data_engine = DataIngestionEngine()\n        portfolio_data = data_engine.ingest_portfolio_data(portfolio_size)\n    \n    st.success(f\"âœ… Successfully ingested data for {len(portfolio_data)} assets\")\n    \n    # Display sample of ingested data\n    df = pd.DataFrame(portfolio_data)\n    st.subheader(\"ðŸ“Š Portfolio Data Preview\")\n    st.dataframe(df.head(10), use_container_width=True)\n    \n    # Store in session state\n    if 'stage_results' not in st.session_state:\n        st.session_state.stage_results = {}\n    st.session_state.stage_results['stage1'] = portfolio_data\n\ndef execute_stage_2():\n    \"\"\"Execute Stage 2: Core Analysis\"\"\"\n    st.header(\"ðŸ” Stage 2: Core Analysis\")\n    \n    if 'stage_results' not in st.session_state or 'stage1' not in st.session_state.stage_results:\n        st.error(\"âŒ Please run Stage 1 first to ingest portfolio data\")\n        return\n    \n    portfolio_data = st.session_state.stage_results['stage1']\n    \n    with st.spinner(\"Running time-series and rule-based analysis...\"):\n        analysis_engine = CoreAnalysisEngine()\n        analysis_results = analysis_engine.analyze_portfolio(portfolio_data)\n    \n    red_flags = len([a for a in analysis_results if a['risk_rating'] == 'RED'])\n    yellow_flags = len([a for a in analysis_results if a['risk_rating'] == 'YELLOW'])\n    green_flags = len([a for a in analysis_results if a['risk_rating'] == 'GREEN'])\n    \n    st.success(f\"âœ… Analysis complete: {red_flags} RED, {yellow_flags} YELLOW, {green_flags} GREEN\")\n    \n    # Display results\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        st.metric(\"ðŸ”´ RED Flags\", red_flags)\n    with col2:\n        st.metric(\"ðŸŸ¡ YELLOW Flags\", yellow_flags)\n    with col3:\n        st.metric(\"ðŸŸ¢ GREEN Assets\", green_flags)\n    \n    # Display flagged assets\n    flagged_assets = [a for a in analysis_results if a['risk_rating'] in ['RED', 'YELLOW']]\n    if flagged_assets:\n        st.subheader(\"âš ï¸ Flagged Assets\")\n        flagged_df = pd.DataFrame(flagged_assets)\n        st.dataframe(flagged_df, use_container_width=True)\n    \n    st.session_state.stage_results['stage2'] = analysis_results\n\ndef execute_stage_3():\n    \"\"\"Execute Stage 3: ML Analysis\"\"\"\n    st.header(\"ðŸ¤– Stage 3: ML Analysis\")\n    \n    if 'stage_results' not in st.session_state or 'stage2' not in st.session_state.stage_results:\n        st.error(\"âŒ Please run Stage 2 first to generate analysis results\")\n        return\n    \n    analysis_results = st.session_state.stage_results['stage2']\n    \n    with st.spinner(\"Running ML analysis (Anomaly Detection & Risk Prediction)...\"):\n        ml_engine = MLAnalysisEngine()\n        ml_results = ml_engine.analyze_portfolio_ml(analysis_results)\n    \n    anomaly_count = ml_results['ml_summary']['anomaly_summary']['total_anomalies']\n    critical_count = ml_results['ml_summary']['anomaly_summary']['critical_anomalies']\n    \n    st.success(f\"âœ… ML analysis complete: {anomaly_count} anomalies detected ({critical_count} critical)\")\n    \n    # Display ML summary\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        st.metric(\"Total Anomalies\", anomaly_count)\n    with col2:\n        st.metric(\"Critical Anomalies\", critical_count)\n    with col3:\n        if ml_results['risk_prediction'].get('model_trained'):\n            st.metric(\"Model Accuracy\", f\"{ml_results['risk_prediction']['test_accuracy']}%\")\n        else:\n            st.metric(\"Model Status\", \"Not Trained\")\n    \n    # Display anomaly results\n    if ml_results['anomaly_detection']:\n        st.subheader(\"ðŸ” Anomaly Detection Results\")\n        anomaly_df = pd.DataFrame(ml_results['anomaly_detection'])\n        st.dataframe(anomaly_df[['symbol', 'sector', 'anomaly_score', 'severity', 'is_anomaly']], use_container_width=True)\n    \n    st.session_state.stage_results['stage3'] = ml_results\n\ndef execute_stage_4():\n    \"\"\"Execute Stage 4: Sentiment Analysis\"\"\"\n    st.header(\"ðŸ“° Stage 4: Sentiment Analysis\")\n    \n    if 'stage_results' not in st.session_state or 'stage2' not in st.session_state.stage_results:\n        st.error(\"âŒ Please run Stage 2 first to identify RED-flagged assets\")\n        return\n    \n    analysis_results = st.session_state.stage_results['stage2']\n    red_flagged_assets = [a for a in analysis_results if a['risk_rating'] == 'RED']\n    \n    if not red_flagged_assets:\n        st.info(\"â„¹ï¸ No RED-flagged assets found. Sentiment analysis not needed.\")\n        return\n    \n    with st.spinner(f\"Analyzing sentiment for {len(red_flagged_assets)} RED-flagged assets...\"):\n        sentiment_engine = SentimentAnalysisEngine()\n        sentiment_results = sentiment_engine.analyze_sentiment(red_flagged_assets)\n    \n    st.success(f\"âœ… Sentiment analysis complete for {len(sentiment_results)} assets\")\n    \n    # Display sentiment results\n    if sentiment_results:\n        st.subheader(\"ðŸ“Š Sentiment Analysis Results\")\n        sentiment_df = pd.DataFrame(sentiment_results)\n        st.dataframe(sentiment_df, use_container_width=True)\n        \n        # Sentiment distribution\n        avg_sentiment = np.mean([s['sentiment_score'] for s in sentiment_results])\n        st.metric(\"ðŸ“ˆ Average Sentiment Score\", f\"{avg_sentiment:.3f}\")\n    \n    st.session_state.stage_results['stage4'] = sentiment_results\n\ndef execute_stage_5():\n    \"\"\"Execute Stage 5: Report Generation\"\"\"\n    st.header(\"ðŸ“„ Stage 5: Report Generation\")\n    \n    required_stages = ['stage1', 'stage2', 'stage3']\n    if 'stage_results' not in st.session_state or not all(stage in st.session_state.stage_results for stage in required_stages):\n        st.error(\"âŒ Please run Stages 1, 2, and 3 first\")\n        return\n    \n    portfolio_data = st.session_state.stage_results['stage1']\n    analysis_results = st.session_state.stage_results['stage2']\n    ml_results = st.session_state.stage_results['stage3']\n    sentiment_results = st.session_state.stage_results.get('stage4', [])\n    \n    with st.spinner(\"Generating comprehensive PDF report and CSV files...\"):\n        report_generator = ReportGenerator()\n        report_files = report_generator.generate_report(\n            portfolio_data, analysis_results, sentiment_results, ml_results\n        )\n    \n    st.success(\"âœ… PDF report and CSV files generated successfully!\")\n    \n    # Display download buttons\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        if os.path.exists(report_files['pdf_path']):\n            with open(report_files['pdf_path'], 'rb') as pdf_file:\n                st.download_button(\n                    label=\"ðŸ“„ Download PDF Report\",\n                    data=pdf_file.read(),\n                    file_name=os.path.basename(report_files['pdf_path']),\n                    mime=\"application/pdf\",\n                    key=\"stage5_pdf\"\n                )\n    \n    with col2:\n        if os.path.exists(report_files['portfolio_csv']):\n            with open(report_files['portfolio_csv'], 'rb') as csv_file:\n                st.download_button(\n                    label=\"ðŸ“Š Download Portfolio CSV\",\n                    data=csv_file.read(),\n                    file_name=os.path.basename(report_files['portfolio_csv']),\n                    mime=\"text/csv\",\n                    key=\"stage5_portfolio_csv\"\n                )\n    \n    with col3:\n        if os.path.exists(report_files['analysis_csv']):\n            with open(report_files['analysis_csv'], 'rb') as csv_file:\n                st.download_button(\n                    label=\"ðŸ“ˆ Download Risk Analysis CSV\",\n                    data=csv_file.read(),\n                    file_name=os.path.basename(report_files['analysis_csv']),\n                    mime=\"text/csv\",\n                    key=\"stage5_analysis_csv\"\n                )\n\ndef display_pipeline_results():\n    \"\"\"Display comprehensive pipeline results\"\"\"\n    st.header(\"ðŸ“ˆ Pipeline Results Summary\")\n    \n    results = st.session_state.pipeline_results\n    \n    # Key metrics\n    col1, col2, col3, col4 = st.columns(4)\n    with col1:\n        st.metric(\"Total Assets\", len(results['portfolio_data']))\n    with col2:\n        st.metric(\"ðŸ”´ RED Flags\", results['red_flags'])\n    with col3:\n        st.metric(\"ðŸŸ¡ YELLOW Flags\", results['yellow_flags'])\n    with col4:\n        st.metric(\"â±ï¸ Execution Time\", f\"{st.session_state.execution_time:.2f}s\")\n    \n    # Tabs for detailed results\n    tab1, tab2, tab3, tab4, tab5 = st.tabs([\"ðŸ“Š Portfolio Overview\", \"âš ï¸ Risk Analysis\", \"ðŸ¤– ML Analysis\", \"ðŸ“° Sentiment\", \"ðŸ“„ Report\"])\n    \n    with tab1:\n        st.subheader(\"Portfolio Data\")\n        df = pd.DataFrame(results['portfolio_data'])\n        st.dataframe(df, use_container_width=True)\n    \n    with tab2:\n        st.subheader(\"Risk Analysis Results\")\n        analysis_df = pd.DataFrame(results['analysis_results'])\n        st.dataframe(analysis_df, use_container_width=True)\n        \n        # Risk distribution chart\n        risk_counts = analysis_df['risk_rating'].value_counts()\n        st.bar_chart(risk_counts)\n    \n    with tab3:\n        st.subheader(\"Machine Learning Analysis\")\n        if 'ml_results' in results and results['ml_results']:\n            ml_data = results['ml_results']\n            \n            # ML Summary\n            st.markdown(\"#### ML Analysis Summary\")\n            summary = ml_data['ml_summary']\n            \n            col1, col2, col3 = st.columns(3)\n            with col1:\n                st.metric(\"Total Anomalies\", summary['anomaly_summary']['total_anomalies'])\n            with col2:\n                st.metric(\"Critical Anomalies\", summary['anomaly_summary']['critical_anomalies'])\n            with col3:\n                if summary['prediction_summary']['model_trained']:\n                    st.metric(\"Model Accuracy\", f\"{summary['prediction_summary']['model_accuracy']}%\")\n                else:\n                    st.metric(\"Model Status\", \"Not Trained\")\n            \n            # Key Insights\n            st.markdown(\"#### Key ML Insights\")\n            for insight in summary['key_insights']:\n                st.info(f\"â€¢ {insight}\")\n            \n            # Validation Results\n            if 'validation' in ml_data:\n                st.markdown(\"#### ML Validation Results\")\n                validation = ml_data['validation']\n                \n                # Overall status\n                if validation['overall_status'] == 'PASS':\n                    st.success(f\"âœ… All validation checks passed ({validation['passed_checks']}/{validation['total_checks']})\")\n                elif validation['overall_status'] == 'WARNING':\n                    st.warning(f\"âš ï¸ Validation completed with warnings ({validation['passed_checks']}/{validation['total_checks']} passed)\")\n                else:\n                    st.error(f\"âŒ Validation failed ({validation['passed_checks']}/{validation['total_checks']} passed)\")\n                \n                # Validation checks details\n                with st.expander(\"View Detailed Validation Checks\"):\n                    for check in validation['validation_checks']:\n                        status_icon = \"âœ…\" if check['status'] == 'PASS' else \"âš ï¸\" if check['status'] == 'WARNING' else \"âŒ\"\n                        st.markdown(f\"**{status_icon} {check['check_name']}** - {check['status']}\")\n                        \n                        if check['metrics']:\n                            st.json(check['metrics'])\n                        \n                        if check['issues']:\n                            for issue in check['issues']:\n                                st.warning(f\"  â€¢ {issue}\")\n                \n                # Display warnings if any\n                if validation['warnings']:\n                    with st.expander(f\"âš ï¸ {len(validation['warnings'])} Validation Warnings\"):\n                        for warning in validation['warnings']:\n                            st.text(f\"â€¢ {warning}\")\n            \n            # Anomaly Detection Results\n            st.markdown(\"#### Anomaly Detection Results\")\n            anomaly_df = pd.DataFrame(ml_data['anomaly_detection'])\n            st.dataframe(anomaly_df[['symbol', 'sector', 'anomaly_score', 'severity', 'is_anomaly', 'recommendation']], use_container_width=True)\n            \n            # Risk Predictions\n            if ml_data['risk_prediction'].get('model_trained'):\n                st.markdown(\"#### Risk Rating Predictions\")\n                pred_df = pd.DataFrame(ml_data['risk_prediction']['predictions'])\n                st.dataframe(pred_df[['symbol', 'current_rating', 'predicted_rating', 'confidence', 'trend']], use_container_width=True)\n            \n            # Feature Importance\n            if ml_data['feature_importance']:\n                st.markdown(\"#### Feature Importance (Risk Drivers)\")\n                importance_df = pd.DataFrame(ml_data['feature_importance'])\n                st.bar_chart(importance_df.set_index('feature')['importance'])\n        else:\n            st.info(\"ML analysis not available\")\n    \n    with tab4:\n        st.subheader(\"Sentiment Analysis\")\n        if results['sentiment_results']:\n            sentiment_df = pd.DataFrame(results['sentiment_results'])\n            st.dataframe(sentiment_df, use_container_width=True)\n        else:\n            st.info(\"No RED-flagged assets required sentiment analysis\")\n    \n    with tab5:\n        st.subheader(\"Generated Report & Data Files\")\n        \n        # Download buttons in columns\n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            if 'pdf_path' in results and os.path.exists(results['pdf_path']):\n                st.success(\"ðŸ“„ PDF Report Ready\")\n                with open(results['pdf_path'], 'rb') as pdf_file:\n                    st.download_button(\n                        label=\"ðŸ“„ Download PDF Report\",\n                        data=pdf_file.read(),\n                        file_name=os.path.basename(results['pdf_path']),\n                        mime=\"application/pdf\",\n                        key=\"tab_pdf\"\n                    )\n            else:\n                st.error(\"PDF not found\")\n        \n        with col2:\n            if 'portfolio_csv' in results and os.path.exists(results['portfolio_csv']):\n                st.success(\"ðŸ“Š Portfolio CSV Ready\")\n                with open(results['portfolio_csv'], 'rb') as csv_file:\n                    st.download_button(\n                        label=\"ðŸ“Š Download Portfolio CSV\",\n                        data=csv_file.read(),\n                        file_name=os.path.basename(results['portfolio_csv']),\n                        mime=\"text/csv\",\n                        key=\"tab_portfolio_csv\"\n                    )\n            else:\n                st.info(\"Portfolio CSV not available\")\n        \n        with col3:\n            if 'analysis_csv' in results and os.path.exists(results['analysis_csv']):\n                st.success(\"ðŸ“ˆ Analysis CSV Ready\")\n                with open(results['analysis_csv'], 'rb') as csv_file:\n                    st.download_button(\n                        label=\"ðŸ“ˆ Download Risk Analysis CSV\",\n                        data=csv_file.read(),\n                        file_name=os.path.basename(results['analysis_csv']),\n                        mime=\"text/csv\",\n                        key=\"tab_analysis_csv\"\n                    )\n            else:\n                st.info(\"Analysis CSV not available\")\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":22966},"pipeline/report_generator.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport os\nfrom typing import List, Dict, Any\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom reportlab.lib import colors\nfrom reportlab.lib.pagesizes import letter, A4\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.units import inch\nfrom reportlab.graphics.shapes import Drawing\nfrom reportlab.graphics.charts.lineplots import LinePlot\nfrom reportlab.graphics.charts.piecharts import Pie\nfrom reportlab.graphics.widgets.markers import makeMarker\nimport io\nimport base64\n\nclass ReportGenerator:\n    \"\"\"\n    Stage 4: Report Generation Engine\n    Generates comprehensive PDF reports with all pipeline findings\n    \"\"\"\n    \n    def __init__(self):\n        self.report_dir = \"reports\"\n        self.charts_dir = \"charts\"\n        self.ensure_directories()\n        \n        # Report styling\n        self.styles = getSampleStyleSheet()\n        self.custom_styles = self.create_custom_styles()\n    \n    def ensure_directories(self):\n        \"\"\"Create necessary directories for reports and charts\"\"\"\n        os.makedirs(self.report_dir, exist_ok=True)\n        os.makedirs(self.charts_dir, exist_ok=True)\n    \n    def create_custom_styles(self):\n        \"\"\"Create custom paragraph styles for the report\"\"\"\n        custom_styles = {}\n        \n        # Title style\n        custom_styles['CustomTitle'] = ParagraphStyle(\n            'CustomTitle',\n            parent=self.styles['Title'],\n            fontSize=24,\n            spaceAfter=30,\n            textColor=colors.darkblue,\n            alignment=1  # Center alignment\n        )\n        \n        # Section header style\n        custom_styles['SectionHeader'] = ParagraphStyle(\n            'SectionHeader',\n            parent=self.styles['Heading1'],\n            fontSize=16,\n            spaceAfter=12,\n            spaceBefore=20,\n            textColor=colors.darkred,\n            borderWidth=1,\n            borderColor=colors.gray,\n            borderPadding=5\n        )\n        \n        # Subsection header style\n        custom_styles['SubsectionHeader'] = ParagraphStyle(\n            'SubsectionHeader',\n            parent=self.styles['Heading2'],\n            fontSize=14,\n            spaceAfter=8,\n            spaceBefore=15,\n            textColor=colors.darkblue\n        )\n        \n        return custom_styles\n    \n    def generate_report(self, portfolio_data: List[Dict], analysis_results: List[Dict], \n                       sentiment_results: List[Dict], ml_results: Dict = None) -> str:\n        \"\"\"\n        Generate comprehensive PDF report\n        \n        Args:\n            portfolio_data: Original portfolio data\n            analysis_results: Risk analysis results\n            sentiment_results: Sentiment analysis results\n            ml_results: Machine learning analysis results (optional)\n            \n        Returns:\n            Path to generated PDF report\n        \"\"\"\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        pdf_filename = f\"portfolio_risk_report_{timestamp}.pdf\"\n        pdf_path = os.path.join(self.report_dir, pdf_filename)\n        \n        # Create PDF document\n        doc = SimpleDocTemplate(pdf_path, pagesize=A4, rightMargin=72, leftMargin=72,\n                              topMargin=72, bottomMargin=18)\n        \n        # Build report content\n        story = []\n        \n        # Title page\n        story.extend(self.create_title_page(portfolio_data, analysis_results))\n        \n        # Executive summary\n        story.extend(self.create_executive_summary(portfolio_data, analysis_results, sentiment_results))\n        \n        # Portfolio overview\n        story.extend(self.create_portfolio_overview(portfolio_data, analysis_results))\n        \n        # Risk analysis section\n        story.extend(self.create_risk_analysis_section(analysis_results))\n        \n        # ML analysis section\n        if ml_results:\n            story.extend(self.create_ml_analysis_section(ml_results))\n        \n        # Sentiment analysis section\n        if sentiment_results:\n            story.extend(self.create_sentiment_analysis_section(sentiment_results))\n        \n        # Detailed asset analysis\n        story.extend(self.create_detailed_analysis_section(analysis_results, sentiment_results))\n        \n        # Recommendations\n        story.extend(self.create_recommendations_section(analysis_results, sentiment_results))\n        \n        # Appendix\n        story.extend(self.create_appendix(portfolio_data, analysis_results))\n        \n        # Export test data to CSV files\n        portfolio_csv, analysis_csv = self.export_test_data_to_csv(portfolio_data, analysis_results, timestamp)\n        \n        # Test Data Section (for further analysis)\n        story.extend(self.create_test_data_section(portfolio_data, analysis_results, portfolio_csv, analysis_csv))\n        \n        # Build PDF\n        doc.build(story)\n        \n        return {\n            'pdf_path': pdf_path,\n            'portfolio_csv': portfolio_csv,\n            'analysis_csv': analysis_csv\n        }\n    \n    def create_title_page(self, portfolio_data: List[Dict], analysis_results: List[Dict]) -> List:\n        \"\"\"Create report title page\"\"\"\n        story = []\n        \n        # Main title\n        title = Paragraph(\"Portfolio Risk Analysis Report\", self.custom_styles['CustomTitle'])\n        story.append(title)\n        story.append(Spacer(1, 30))\n        \n        # Report metadata\n        report_date = datetime.now().strftime('%B %d, %Y')\n        metadata_text = f\"\"\"\n        <para align=center>\n        <b>Report Date:</b> {report_date}<br/>\n        <b>Portfolio Size:</b> {len(portfolio_data)} Assets<br/>\n        <b>Analysis Period:</b> 12 Months<br/>\n        <b>Generated by:</b> Automated Risk Analysis Pipeline\n        </para>\n        \"\"\"\n        story.append(Paragraph(metadata_text, self.styles['Normal']))\n        story.append(Spacer(1, 50))\n        \n        # Key statistics\n        red_count = len([a for a in analysis_results if a['risk_rating'] == 'RED'])\n        yellow_count = len([a for a in analysis_results if a['risk_rating'] == 'YELLOW'])\n        green_count = len([a for a in analysis_results if a['risk_rating'] == 'GREEN'])\n        \n        stats_text = f\"\"\"\n        <para align=center fontSize=14>\n        <b>Risk Assessment Summary</b><br/><br/>\n        <font color=red><b>HIGH RISK (RED):</b> {red_count} Assets</font><br/>\n        <font color=orange><b>MEDIUM RISK (YELLOW):</b> {yellow_count} Assets</font><br/>\n        <font color=green><b>LOW RISK (GREEN):</b> {green_count} Assets</font>\n        </para>\n        \"\"\"\n        story.append(Paragraph(stats_text, self.styles['Normal']))\n        \n        story.append(PageBreak())\n        return story\n    \n    def create_executive_summary(self, portfolio_data: List[Dict], \n                               analysis_results: List[Dict], sentiment_results: List[Dict]) -> List:\n        \"\"\"Create executive summary section\"\"\"\n        story = []\n        \n        story.append(Paragraph(\"Executive Summary\", self.custom_styles['SectionHeader']))\n        \n        # Calculate key metrics\n        total_assets = len(portfolio_data)\n        total_market_cap = sum(asset['market_cap'] for asset in portfolio_data)\n        \n        red_assets = [a for a in analysis_results if a['risk_rating'] == 'RED']\n        yellow_assets = [a for a in analysis_results if a['risk_rating'] == 'YELLOW']\n        \n        avg_volatility = np.mean([a['volatility'] for a in analysis_results])\n        max_drawdown_portfolio = min([a['max_drawdown'] for a in analysis_results])\n        \n        # Executive summary text\n        summary_text = f\"\"\"\n        This report presents a comprehensive risk analysis of a portfolio containing {total_assets} assets \n        with a combined market capitalization of ${total_market_cap/1e9:.1f} billion.\n        \n        <b>Key Findings:</b>\n        â€¢ {len(red_assets)} assets ({len(red_assets)/total_assets*100:.1f}%) are rated as HIGH RISK (RED)\n        â€¢ {len(yellow_assets)} assets ({len(yellow_assets)/total_assets*100:.1f}%) are rated as MEDIUM RISK (YELLOW)\n        â€¢ Average portfolio volatility: {avg_volatility*100:.1f}%\n        â€¢ Maximum drawdown observed: {max_drawdown_portfolio*100:.1f}%\n        \n        <b>Risk Concentration:</b>\n        The analysis reveals significant risk concentration in {len(red_assets + yellow_assets)} assets \n        requiring immediate attention and potential portfolio rebalancing.\n        \"\"\"\n        \n        if sentiment_results:\n            avg_sentiment = np.mean([s['sentiment_score'] for s in sentiment_results])\n            negative_sentiment_count = len([s for s in sentiment_results if s['sentiment_label'] == 'NEGATIVE'])\n            \n            summary_text += f\"\"\"\n            \n            <b>Sentiment Analysis:</b>\n            â€¢ {negative_sentiment_count} RED-flagged assets show negative market sentiment\n            â€¢ Average sentiment score for flagged assets: {avg_sentiment:.3f}\n            â€¢ News coverage indicates heightened market concern for these positions\n            \"\"\"\n        \n        story.append(Paragraph(summary_text, self.styles['Normal']))\n        story.append(Spacer(1, 20))\n        \n        return story\n    \n    def create_portfolio_overview(self, portfolio_data: List[Dict], analysis_results: List[Dict]) -> List:\n        \"\"\"Create portfolio overview section\"\"\"\n        story = []\n        \n        story.append(Paragraph(\"Portfolio Overview\", self.custom_styles['SectionHeader']))\n        \n        # Create sector allocation table\n        sector_data = {}\n        for asset in portfolio_data:\n            sector = asset['sector']\n            if sector not in sector_data:\n                sector_data[sector] = {'count': 0, 'market_cap': 0}\n            sector_data[sector]['count'] += 1\n            sector_data[sector]['market_cap'] += asset['market_cap']\n        \n        # Sector allocation table\n        story.append(Paragraph(\"Sector Allocation\", self.custom_styles['SubsectionHeader']))\n        \n        sector_table_data = [['Sector', 'Assets', 'Market Cap ($B)', 'Percentage']]\n        total_market_cap = sum(data['market_cap'] for data in sector_data.values())\n        \n        for sector, data in sorted(sector_data.items(), key=lambda x: x[1]['market_cap'], reverse=True):\n            percentage = data['market_cap'] / total_market_cap * 100\n            sector_table_data.append([\n                sector,\n                str(data['count']),\n                f\"{data['market_cap']/1e9:.2f}\",\n                f\"{percentage:.1f}%\"\n            ])\n        \n        sector_table = Table(sector_table_data)\n        sector_table.setStyle(TableStyle([\n            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n            ('FONTSIZE', (0, 0), (-1, 0), 14),\n            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n            ('GRID', (0, 0), (-1, -1), 1, colors.black)\n        ]))\n        \n        story.append(sector_table)\n        story.append(Spacer(1, 20))\n        \n        return story\n    \n    def create_risk_analysis_section(self, analysis_results: List[Dict]) -> List:\n        \"\"\"Create risk analysis section\"\"\"\n        story = []\n        \n        story.append(Paragraph(\"Risk Analysis Results\", self.custom_styles['SectionHeader']))\n        \n        # Risk distribution summary\n        risk_counts = {'RED': 0, 'YELLOW': 0, 'GREEN': 0}\n        for result in analysis_results:\n            risk_counts[result['risk_rating']] += 1\n        \n        risk_text = f\"\"\"\n        <b>Risk Distribution:</b><br/>\n        â€¢ HIGH RISK (RED): {risk_counts['RED']} assets<br/>\n        â€¢ MEDIUM RISK (YELLOW): {risk_counts['YELLOW']} assets<br/>\n        â€¢ LOW RISK (GREEN): {risk_counts['GREEN']} assets<br/>\n        \"\"\"\n        story.append(Paragraph(risk_text, self.styles['Normal']))\n        story.append(Spacer(1, 15))\n        \n        # High risk assets table\n        high_risk_assets = [a for a in analysis_results if a['risk_rating'] in ['RED', 'YELLOW']]\n        \n        if high_risk_assets:\n            story.append(Paragraph(\"High Risk Assets\", self.custom_styles['SubsectionHeader']))\n            \n            risk_table_data = [['Symbol', 'Sector', 'Risk Rating', 'Volatility', 'Max Drawdown', 'Risk Score']]\n            \n            for asset in sorted(high_risk_assets, key=lambda x: x['risk_score'], reverse=True):\n                risk_table_data.append([\n                    asset['symbol'],\n                    asset['sector'],\n                    asset['risk_rating'],\n                    f\"{asset['volatility']*100:.1f}%\",\n                    f\"{asset['max_drawdown']*100:.1f}%\",\n                    str(asset['risk_score'])\n                ])\n            \n            risk_table = Table(risk_table_data)\n            risk_table.setStyle(TableStyle([\n                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n                ('FONTSIZE', (0, 0), (-1, 0), 12),\n                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n                ('GRID', (0, 0), (-1, -1), 1, colors.black)\n            ]))\n            \n            # Color code risk ratings\n            for i, asset in enumerate(high_risk_assets, 1):\n                if asset['risk_rating'] == 'RED':\n                    risk_table.setStyle(TableStyle([('BACKGROUND', (2, i), (2, i), colors.lightcoral)]))\n                elif asset['risk_rating'] == 'YELLOW':\n                    risk_table.setStyle(TableStyle([('BACKGROUND', (2, i), (2, i), colors.lightyellow)]))\n            \n            story.append(risk_table)\n        \n        story.append(Spacer(1, 20))\n        return story\n    \n    def create_sentiment_analysis_section(self, sentiment_results: List[Dict]) -> List:\n        \"\"\"Create sentiment analysis section\"\"\"\n        story = []\n        \n        story.append(Paragraph(\"Sentiment Analysis\", self.custom_styles['SectionHeader']))\n        \n        if not sentiment_results:\n            story.append(Paragraph(\"No RED-flagged assets required sentiment analysis.\", self.styles['Normal']))\n            return story\n        \n        # Sentiment summary\n        avg_sentiment = np.mean([s['sentiment_score'] for s in sentiment_results])\n        negative_count = len([s for s in sentiment_results if s['sentiment_label'] == 'NEGATIVE'])\n        \n        sentiment_text = f\"\"\"\n        Sentiment analysis was conducted on {len(sentiment_results)} RED-flagged assets using \n        financial news from the past 12 months.\n        \n        <b>Key Findings:</b><br/>\n        â€¢ Average sentiment score: {avg_sentiment:.3f}<br/>\n        â€¢ Assets with negative sentiment: {negative_count}/{len(sentiment_results)}<br/>\n        â€¢ Total news articles analyzed: {sum(s['news_count'] for s in sentiment_results)}<br/>\n        \"\"\"\n        story.append(Paragraph(sentiment_text, self.styles['Normal']))\n        story.append(Spacer(1, 15))\n        \n        # Sentiment results table\n        story.append(Paragraph(\"Sentiment Analysis Results\", self.custom_styles['SubsectionHeader']))\n        \n        sentiment_table_data = [['Symbol', 'Sentiment Score', 'Label', 'News Count', 'Trend', 'Key Themes']]\n        \n        for result in sorted(sentiment_results, key=lambda x: x['sentiment_score']):\n            key_themes = ', '.join(result['key_themes'][:3]) if result['key_themes'] else 'None'\n            sentiment_table_data.append([\n                result['symbol'],\n                f\"{result['sentiment_score']:.3f}\",\n                result['sentiment_label'],\n                str(result['news_count']),\n                result['sentiment_trend'],\n                key_themes\n            ])\n        \n        sentiment_table = Table(sentiment_table_data)\n        sentiment_table.setStyle(TableStyle([\n            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n            ('FONTSIZE', (0, 0), (-1, 0), 10),\n            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n            ('GRID', (0, 0), (-1, -1), 1, colors.black)\n        ]))\n        \n        story.append(sentiment_table)\n        story.append(Spacer(1, 20))\n        \n        return story\n    \n    def create_ml_analysis_section(self, ml_results: Dict) -> List:\n        \"\"\"Create machine learning analysis section\"\"\"\n        story = []\n        \n        story.append(Paragraph(\"Machine Learning Analysis\", self.custom_styles['SectionHeader']))\n        \n        ml_summary = ml_results['ml_summary']\n        \n        # ML Overview\n        ml_text = f\"\"\"\n        Advanced machine learning techniques were applied to identify anomalies and predict future risk ratings.\n        \n        <b>Anomaly Detection Summary:</b><br/>\n        â€¢ Total Anomalies Detected: {ml_summary['anomaly_summary']['total_anomalies']}<br/>\n        â€¢ Critical Anomalies: {ml_summary['anomaly_summary']['critical_anomalies']}<br/>\n        â€¢ High Risk Anomalies: {ml_summary['anomaly_summary']['high_anomalies']}<br/>\n        â€¢ Anomaly Rate: {ml_summary['anomaly_summary']['anomaly_rate']}%<br/>\n        \"\"\"\n        \n        if ml_summary['prediction_summary']['model_trained']:\n            ml_text += f\"\"\"\n            \n            <b>Risk Prediction Model:</b><br/>\n            â€¢ Model Accuracy: {ml_summary['prediction_summary']['model_accuracy']}%<br/>\n            â€¢ Rating Changes Predicted: {ml_summary['prediction_summary']['rating_changes_predicted']}<br/>\n            â€¢ Assets Predicted to Deteriorate: {ml_summary['prediction_summary']['deteriorating_assets']}<br/>\n            â€¢ Assets Predicted to Improve: {ml_summary['prediction_summary']['improving_assets']}<br/>\n            \"\"\"\n        \n        story.append(Paragraph(ml_text, self.styles['Normal']))\n        story.append(Spacer(1, 15))\n        \n        # Key ML Insights\n        if ml_summary['key_insights']:\n            story.append(Paragraph(\"Key Machine Learning Insights\", self.custom_styles['SubsectionHeader']))\n            insights_text = \"<br/>\".join([f\"â€¢ {insight}\" for insight in ml_summary['key_insights']])\n            story.append(Paragraph(insights_text, self.styles['Normal']))\n            story.append(Spacer(1, 15))\n        \n        # Anomaly Detection Results Table\n        anomaly_results = ml_results['anomaly_detection']\n        critical_anomalies = [a for a in anomaly_results if a['severity'] in ['CRITICAL', 'HIGH']]\n        \n        if critical_anomalies:\n            story.append(Paragraph(\"Critical Anomalies Detected\", self.custom_styles['SubsectionHeader']))\n            \n            anomaly_table_data = [['Symbol', 'Sector', 'Anomaly Score', 'Severity', 'Recommendation']]\n            \n            for anomaly in sorted(critical_anomalies, key=lambda x: x['anomaly_score'], reverse=True)[:10]:\n                anomaly_table_data.append([\n                    anomaly['symbol'],\n                    anomaly['sector'],\n                    f\"{anomaly['anomaly_score']:.1f}\",\n                    anomaly['severity'],\n                    anomaly['recommendation'][:40] + '...' if len(anomaly['recommendation']) > 40 else anomaly['recommendation']\n                ])\n            \n            anomaly_table = Table(anomaly_table_data, colWidths=[60, 80, 70, 60, 180])\n            anomaly_table.setStyle(TableStyle([\n                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n                ('FONTSIZE', (0, 0), (-1, 0), 10),\n                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n                ('GRID', (0, 0), (-1, -1), 1, colors.black),\n                ('FONTSIZE', (0, 1), (-1, -1), 8)\n            ]))\n            \n            story.append(anomaly_table)\n            story.append(Spacer(1, 15))\n        \n        # Risk Predictions Table\n        if ml_results['risk_prediction'].get('model_trained'):\n            predictions = ml_results['risk_prediction']['predictions']\n            rating_changes = [p for p in predictions if p['rating_change']]\n            \n            if rating_changes:\n                story.append(Paragraph(\"Predicted Risk Rating Changes\", self.custom_styles['SubsectionHeader']))\n                \n                pred_table_data = [['Symbol', 'Current Rating', 'Predicted Rating', 'Confidence', 'Trend']]\n                \n                for pred in sorted(rating_changes, key=lambda x: x['confidence'], reverse=True)[:10]:\n                    pred_table_data.append([\n                        pred['symbol'],\n                        pred['current_rating'],\n                        pred['predicted_rating'],\n                        f\"{pred['confidence']:.1f}%\",\n                        pred['trend']\n                    ])\n                \n                pred_table = Table(pred_table_data)\n                pred_table.setStyle(TableStyle([\n                    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n                    ('FONTSIZE', (0, 0), (-1, 0), 10),\n                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n                    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n                ]))\n                \n                story.append(pred_table)\n                story.append(Spacer(1, 15))\n        \n        # Feature Importance\n        if ml_results['feature_importance']:\n            story.append(Paragraph(\"Top Risk Factors (Feature Importance)\", self.custom_styles['SubsectionHeader']))\n            \n            top_features = ml_results['feature_importance'][:5]\n            features_text = \"<br/>\".join([\n                f\"{i+1}. {f['feature']}: {f['importance']:.1f}% importance\" \n                for i, f in enumerate(top_features)\n            ])\n            story.append(Paragraph(features_text, self.styles['Normal']))\n            story.append(Spacer(1, 15))\n        \n        # Validation Results\n        if 'validation' in ml_results:\n            story.append(Paragraph(\"ML Validation Results\", self.custom_styles['SubsectionHeader']))\n            validation = ml_results['validation']\n            \n            # Overall validation status\n            status_color = 'green' if validation['overall_status'] == 'PASS' else 'orange' if validation['overall_status'] == 'WARNING' else 'red'\n            validation_text = f\"\"\"\n            <b>Overall Validation Status:</b> <font color=\"{status_color}\">{validation['overall_status']}</font><br/>\n            <b>Checks Passed:</b> {validation['passed_checks']} / {validation['total_checks']}<br/>\n            \"\"\"\n            \n            # Add validation check summaries\n            for check in validation['validation_checks']:\n                check_status = 'âœ“' if check['status'] == 'PASS' else 'âš ' if check['status'] == 'WARNING' else 'âœ—'\n                validation_text += f\"<br/><b>{check_status} {check['check_name']}:</b> {check['status']}\"\n                \n                if check['metrics']:\n                    metrics_str = ', '.join([f\"{k}: {v}\" for k, v in list(check['metrics'].items())[:3]])\n                    validation_text += f\"<br/>  {metrics_str}\"\n            \n            # Add warnings if any\n            if validation['warnings']:\n                validation_text += f\"<br/><br/><b>Validation Warnings ({len(validation['warnings'])}):</b><br/>\"\n                for i, warning in enumerate(validation['warnings'][:5], 1):\n                    validation_text += f\"{i}. {warning}<br/>\"\n                if len(validation['warnings']) > 5:\n                    validation_text += f\"... and {len(validation['warnings']) - 5} more warnings<br/>\"\n            \n            story.append(Paragraph(validation_text, self.styles['Normal']))\n            story.append(Spacer(1, 15))\n        \n        story.append(Spacer(1, 20))\n        return story\n    \n    def create_detailed_analysis_section(self, analysis_results: List[Dict], \n                                       sentiment_results: List[Dict]) -> List:\n        \"\"\"Create detailed analysis section for top risk assets\"\"\"\n        story = []\n        \n        story.append(Paragraph(\"Detailed Asset Analysis\", self.custom_styles['SectionHeader']))\n        \n        # Focus on top 5 highest risk assets\n        high_risk_assets = sorted(\n            [a for a in analysis_results if a['risk_rating'] in ['RED', 'YELLOW']],\n            key=lambda x: x['risk_score'],\n            reverse=True\n        )[:5]\n        \n        sentiment_dict = {s['symbol']: s for s in sentiment_results}\n        \n        for asset in high_risk_assets:\n            story.append(Paragraph(f\"Asset: {asset['symbol']}\", self.custom_styles['SubsectionHeader']))\n            \n            # Basic information\n            basic_info = f\"\"\"\n            <b>Sector:</b> {asset['sector']}<br/>\n            <b>Current Price:</b> ${asset['current_price']:.2f}<br/>\n            <b>Market Cap:</b> ${asset['market_cap']/1e9:.2f}B<br/>\n            <b>Risk Rating:</b> {asset['risk_rating']}<br/>\n            \"\"\"\n            story.append(Paragraph(basic_info, self.styles['Normal']))\n            \n            # Risk metrics\n            risk_metrics = f\"\"\"\n            <b>Risk Metrics:</b><br/>\n            â€¢ Volatility: {asset['volatility']*100:.1f}%<br/>\n            â€¢ Maximum Drawdown: {asset['max_drawdown']*100:.1f}%<br/>\n            â€¢ Beta: {asset['beta']:.2f}<br/>\n            â€¢ Sharpe Ratio: {asset['sharpe_ratio']:.2f}<br/>\n            â€¢ RSI: {asset['rsi']:.1f}<br/>\n            \"\"\"\n            story.append(Paragraph(risk_metrics, self.styles['Normal']))\n            \n            # Performance metrics\n            performance = f\"\"\"\n            <b>Performance:</b><br/>\n            â€¢ 1-Month Return: {asset['price_change_1m']*100:.1f}%<br/>\n            â€¢ 3-Month Return: {asset['price_change_3m']*100:.1f}%<br/>\n            â€¢ 6-Month Return: {asset['price_change_6m']*100:.1f}%<br/>\n            \"\"\"\n            story.append(Paragraph(performance, self.styles['Normal']))\n            \n            # Risk flags\n            risk_flags = [flag for flag, value in asset['risk_flags'].items() if value]\n            if risk_flags:\n                flags_text = f\"\"\"\n                <b>Risk Flags:</b><br/>\n                â€¢ {', '.join(flag.replace('_', ' ').title() for flag in risk_flags)}<br/>\n                \"\"\"\n                story.append(Paragraph(flags_text, self.styles['Normal']))\n            \n            # Sentiment information if available\n            if asset['symbol'] in sentiment_dict:\n                sentiment_info = sentiment_dict[asset['symbol']]\n                sentiment_text = f\"\"\"\n                <b>Market Sentiment:</b><br/>\n                â€¢ Sentiment Score: {sentiment_info['sentiment_score']:.3f} ({sentiment_info['sentiment_label']})<br/>\n                â€¢ News Articles: {sentiment_info['news_count']}<br/>\n                â€¢ Trend: {sentiment_info['sentiment_trend']}<br/>\n                \"\"\"\n                story.append(Paragraph(sentiment_text, self.styles['Normal']))\n            \n            story.append(Spacer(1, 15))\n        \n        return story\n    \n    def create_recommendations_section(self, analysis_results: List[Dict], \n                                     sentiment_results: List[Dict]) -> List:\n        \"\"\"Create recommendations section\"\"\"\n        story = []\n        \n        story.append(Paragraph(\"Recommendations\", self.custom_styles['SectionHeader']))\n        \n        red_assets = [a for a in analysis_results if a['risk_rating'] == 'RED']\n        yellow_assets = [a for a in analysis_results if a['risk_rating'] == 'YELLOW']\n        \n        recommendations_text = f\"\"\"\n        Based on the comprehensive risk analysis, the following recommendations are provided:\n        \n        <b>Immediate Actions (RED-flagged assets):</b><br/>\n        1. Consider reducing position sizes for {len(red_assets)} high-risk assets<br/>\n        2. Implement stop-loss orders to limit further downside exposure<br/>\n        3. Review fundamental analysis for potential divestiture candidates<br/>\n        4. Monitor daily price movements and news flow closely<br/>\n        \n        <b>Medium-term Actions (YELLOW-flagged assets):</b><br/>\n        1. Conduct deeper due diligence on {len(yellow_assets)} medium-risk assets<br/>\n        2. Consider hedging strategies for positions with high volatility<br/>\n        3. Review correlation with overall portfolio risk<br/>\n        4. Set up enhanced monitoring and alerts<br/>\n        \n        <b>Portfolio-level Recommendations:</b><br/>\n        1. Diversify across sectors to reduce concentration risk<br/>\n        2. Consider alternative investments to reduce correlation<br/>\n        3. Implement systematic risk management framework<br/>\n        4. Schedule monthly portfolio risk reviews<br/>\n        \"\"\"\n        \n        if sentiment_results:\n            negative_sentiment_assets = [s for s in sentiment_results if s['sentiment_label'] == 'NEGATIVE']\n            recommendations_text += f\"\"\"\n            \n            <b>Sentiment-based Actions:</b><br/>\n            1. Monitor news flow for {len(negative_sentiment_assets)} assets with negative sentiment<br/>\n            2. Consider contrarian opportunities if fundamentals remain strong<br/>\n            3. Assess impact of market sentiment on price movements<br/>\n            4. Review analyst coverage and institutional positioning<br/>\n            \"\"\"\n        \n        story.append(Paragraph(recommendations_text, self.styles['Normal']))\n        story.append(Spacer(1, 20))\n        \n        return story\n    \n    def create_appendix(self, portfolio_data: List[Dict], analysis_results: List[Dict]) -> List:\n        \"\"\"Create appendix with methodology and data details\"\"\"\n        story = []\n        \n        story.append(PageBreak())\n        story.append(Paragraph(\"Appendix\", self.custom_styles['SectionHeader']))\n        \n        # Methodology\n        story.append(Paragraph(\"Methodology\", self.custom_styles['SubsectionHeader']))\n        \n        methodology_text = \"\"\"\n        This risk analysis employs a four-stage pipeline methodology:\n        \n        <b>Stage 1 - Data Ingestion:</b>\n        â€¢ Historical price data (252 trading days)\n        â€¢ Trading volume information\n        â€¢ Market capitalization data\n        â€¢ Sector classifications\n        \n        <b>Stage 2 - Core Analysis:</b>\n        â€¢ Volatility calculation (annualized)\n        â€¢ Maximum drawdown analysis\n        â€¢ Beta coefficient estimation\n        â€¢ Risk-adjusted return metrics (Sharpe ratio)\n        â€¢ Technical indicators (RSI)\n        â€¢ Rule-based risk flagging system\n        \n        <b>Stage 3 - Sentiment Analysis:</b>\n        â€¢ News article collection (12-month lookback)\n        â€¢ Natural language processing for sentiment scoring\n        â€¢ Trend analysis and confidence metrics\n        â€¢ Key theme extraction\n        \n        <b>Stage 4 - Report Generation:</b>\n        â€¢ Comprehensive risk assessment compilation\n        â€¢ Visual data presentation\n        â€¢ Actionable recommendations\n        â€¢ Professional PDF report output\n        \"\"\"\n        \n        story.append(Paragraph(methodology_text, self.styles['Normal']))\n        story.append(Spacer(1, 20))\n        \n        # Risk thresholds\n        story.append(Paragraph(\"Risk Assessment Thresholds\", self.custom_styles['SubsectionHeader']))\n        \n        thresholds_text = \"\"\"\n        <b>RED Flag Thresholds:</b>\n        â€¢ Volatility > 40% (annualized)\n        â€¢ Maximum drawdown < -20%\n        â€¢ Volume decline > 50%\n        â€¢ 1-month price decline > 15%\n        \n        <b>YELLOW Flag Thresholds:</b>\n        â€¢ Volatility > 25% (annualized)\n        â€¢ Maximum drawdown < -10%\n        â€¢ Volume decline > 30%\n        â€¢ Multiple warning indicators present\n        \n        <b>Sentiment Thresholds:</b>\n        â€¢ Negative sentiment: Score < -0.3\n        â€¢ Positive sentiment: Score > 0.3\n        â€¢ Neutral sentiment: -0.3 â‰¤ Score â‰¤ 0.3\n        \"\"\"\n        \n        story.append(Paragraph(thresholds_text, self.styles['Normal']))\n        story.append(Spacer(1, 20))\n        \n        # Report generation details\n        report_details = f\"\"\"\n        <b>Report Generation Details:</b><br/>\n        â€¢ Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br/>\n        â€¢ Analysis Period: 12 months<br/>\n        â€¢ Total Assets Analyzed: {len(portfolio_data)}<br/>\n        â€¢ Pipeline Version: 1.0<br/>\n        \"\"\"\n        \n        story.append(Paragraph(report_details, self.styles['Normal']))\n        \n        return story\n    \n    def export_test_data_to_csv(self, portfolio_data: List[Dict], analysis_results: List[Dict], timestamp: str):\n        \"\"\"Export complete portfolio and risk analysis data to CSV files\"\"\"\n        \n        # Export Portfolio Data to CSV\n        portfolio_df = pd.DataFrame(portfolio_data)\n        portfolio_csv_path = os.path.join(self.report_dir, f\"portfolio_data_{timestamp}.csv\")\n        \n        # Select and order columns for portfolio CSV\n        portfolio_columns = ['symbol', 'company_name', 'sector', 'current_price', 'market_cap', \n                           'pe_ratio', 'dividend_yield', 'exchange', 'currency']\n        portfolio_export = portfolio_df[portfolio_columns].copy()\n        portfolio_export = portfolio_export.sort_values('market_cap', ascending=False)\n        portfolio_export.to_csv(portfolio_csv_path, index=False)\n        \n        # Export Risk Analysis Data to CSV\n        analysis_df = pd.DataFrame(analysis_results)\n        analysis_csv_path = os.path.join(self.report_dir, f\"risk_analysis_{timestamp}.csv\")\n        \n        # Select and order columns for risk analysis CSV\n        analysis_columns = ['symbol', 'sector', 'risk_rating', 'risk_score', 'volatility', \n                          'max_drawdown', 'volume_decline', 'beta', 'sharpe_ratio', 'rsi',\n                          'price_change_1m', 'price_change_3m', 'price_change_6m']\n        analysis_export = analysis_df[analysis_columns].copy()\n        analysis_export = analysis_export.sort_values('risk_score', ascending=False)\n        analysis_export.to_csv(analysis_csv_path, index=False)\n        \n        return portfolio_csv_path, analysis_csv_path\n    \n    def create_test_data_section(self, portfolio_data: List[Dict], analysis_results: List[Dict], \n                                 portfolio_csv: str, analysis_csv: str) -> List:\n        \"\"\"Create test data section with CSV references and summary tables for further analysis\"\"\"\n        story = []\n        \n        story.append(PageBreak())\n        story.append(Paragraph(\"Test Data - Portfolio Details\", self.custom_styles['SectionHeader']))\n        \n        # Introduction text with CSV file references\n        intro_text = f\"\"\"\n        Complete portfolio and risk analysis data has been exported to CSV files for further analysis:\n        <br/><br/>\n        <b>Portfolio Data CSV:</b> {os.path.basename(portfolio_csv)}<br/>\n        <b>Risk Analysis CSV:</b> {os.path.basename(analysis_csv)}<br/>\n        <br/>\n        The tables below provide summary information for quick reference.\n        \"\"\"\n        story.append(Paragraph(intro_text, self.styles['Normal']))\n        story.append(Spacer(1, 15))\n        \n        # Performance Metrics Table\n        story.append(Paragraph(\"Performance Metrics Data\", self.custom_styles['SubsectionHeader']))\n        \n        performance_table_data = [['Symbol', '1M Return', '3M Return', '6M Return', 'Vol Decline', 'Sharpe Ratio']]\n        \n        for result in sorted(analysis_results, key=lambda x: x['symbol']):\n            performance_table_data.append([\n                result['symbol'],\n                f\"{result['price_change_1m']*100:.1f}%\",\n                f\"{result['price_change_3m']*100:.1f}%\",\n                f\"{result['price_change_6m']*100:.1f}%\",\n                f\"{result['volume_decline']*100:.1f}%\",\n                f\"{result['sharpe_ratio']:.2f}\"\n            ])\n        \n        performance_table = Table(performance_table_data)\n        performance_table.setStyle(TableStyle([\n            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n            ('FONTSIZE', (0, 0), (-1, 0), 9),\n            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n            ('FONTSIZE', (0, 1), (-1, -1), 8)\n        ]))\n        \n        story.append(performance_table)\n        story.append(Spacer(1, 20))\n        \n        # Risk Flags Details Table\n        story.append(Paragraph(\"Risk Flags Details\", self.custom_styles['SubsectionHeader']))\n        \n        risk_flags_table_data = [['Symbol', 'High Vol', 'Ext. DD', 'Vol Collapse', 'Severe Dec', 'Ext. Dec', 'Poor Sharpe', 'Mom. Break']]\n        \n        for result in sorted(analysis_results, key=lambda x: x['risk_score'], reverse=True):\n            flags = result['risk_flags']\n            risk_flags_table_data.append([\n                result['symbol'],\n                'âœ“' if flags.get('high_volatility') else 'âœ—',\n                'âœ“' if flags.get('extreme_drawdown') else 'âœ—',\n                'âœ“' if flags.get('volume_collapse') else 'âœ—',\n                'âœ“' if flags.get('severe_decline') else 'âœ—',\n                'âœ“' if flags.get('extended_decline') else 'âœ—',\n                'âœ“' if flags.get('poor_risk_return') else 'âœ—',\n                'âœ“' if flags.get('momentum_breakdown') else 'âœ—'\n            ])\n        \n        risk_flags_table = Table(risk_flags_table_data)\n        risk_flags_table.setStyle(TableStyle([\n            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n            ('FONTSIZE', (0, 0), (-1, 0), 8),\n            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n            ('FONTSIZE', (0, 1), (-1, -1), 7)\n        ]))\n        \n        story.append(risk_flags_table)\n        story.append(Spacer(1, 20))\n        \n        # Data summary note\n        summary_note = f\"\"\"\n        <b>Data Summary:</b><br/>\n        â€¢ Total Assets: {len(portfolio_data)}<br/>\n        â€¢ Data Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br/>\n        â€¢ Note: This test data is provided for further analysis, validation, and detailed review purposes.<br/>\n        â€¢ All metrics are calculated from 252 trading days of historical data.\n        \"\"\"\n        story.append(Paragraph(summary_note, self.styles['Normal']))\n        \n        return story\n","size_bytes":39985},"pipeline/core_analysis.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass CoreAnalysisEngine:\n    \"\"\"\n    Stage 2: Core Analysis Engine\n    Performs time-series and rule-based analysis to determine Asset Quality Ratings\n    \"\"\"\n    \n    def __init__(self):\n        self.risk_thresholds = {\n            'volatility_red': 0.4,      # 40% annualized volatility\n            'volatility_yellow': 0.25,   # 25% annualized volatility\n            'drawdown_red': -0.2,        # -20% maximum drawdown\n            'drawdown_yellow': -0.1,     # -10% maximum drawdown\n            'volume_decline_red': -0.5,  # -50% volume decline\n            'volume_decline_yellow': -0.3, # -30% volume decline\n            'correlation_threshold': 0.8  # High correlation threshold\n        }\n    \n    def analyze_portfolio(self, portfolio_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Analyze entire portfolio and generate risk ratings\n        \n        Args:\n            portfolio_data: List of asset data dictionaries\n            \n        Returns:\n            List of analysis results with risk ratings\n        \"\"\"\n        analysis_results = []\n        \n        # Convert to DataFrame for easier analysis\n        portfolio_df = pd.DataFrame(portfolio_data)\n        \n        for asset in portfolio_data:\n            asset_analysis = self.analyze_single_asset(asset)\n            \n            # Add portfolio-level metrics\n            asset_analysis.update({\n                'portfolio_weight': asset['market_cap'] / portfolio_df['market_cap'].sum(),\n                'analysis_timestamp': datetime.now().isoformat()\n            })\n            \n            analysis_results.append(asset_analysis)\n        \n        # Add correlation analysis\n        self.add_correlation_analysis(analysis_results, portfolio_data)\n        \n        return analysis_results\n    \n    def analyze_single_asset(self, asset_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Perform comprehensive analysis on a single asset\n        \n        Args:\n            asset_data: Dictionary containing asset information\n            \n        Returns:\n            Analysis results for the asset\n        \"\"\"\n        prices = np.array(asset_data['historical_prices'])\n        volumes = np.array(asset_data['trading_volume_history'])\n        \n        # Calculate key metrics\n        returns = np.diff(prices) / prices[:-1]\n        volatility = np.std(returns) * np.sqrt(252)  # Annualized volatility\n        \n        # Maximum drawdown calculation\n        cumulative_returns = np.cumprod(1 + returns)\n        running_max = np.maximum.accumulate(cumulative_returns)\n        drawdown = (cumulative_returns - running_max) / running_max\n        max_drawdown = np.min(drawdown)\n        \n        # Volume analysis\n        volume_ma_short = np.mean(volumes[-20:])  # 20-day average\n        volume_ma_long = np.mean(volumes[-60:])   # 60-day average\n        volume_decline = (volume_ma_short - volume_ma_long) / volume_ma_long if volume_ma_long != 0 else 0\n        \n        # Price momentum\n        price_change_1m = (prices[-1] - prices[-21]) / prices[-21] if len(prices) >= 21 else 0\n        price_change_3m = (prices[-1] - prices[-63]) / prices[-63] if len(prices) >= 63 else 0\n        price_change_6m = (prices[-1] - prices[-126]) / prices[-126] if len(prices) >= 126 else 0\n        \n        # Sharpe ratio (assuming risk-free rate of 2%)\n        risk_free_rate = 0.02\n        excess_returns = np.mean(returns) * 252 - risk_free_rate\n        sharpe_ratio = excess_returns / volatility if volatility != 0 else 0\n        \n        # Technical indicators\n        rsi = self.calculate_rsi(prices)\n        beta = self.calculate_beta(returns)\n        \n        # Risk flag calculation\n        risk_flags = self.calculate_risk_flags(\n            volatility, max_drawdown, volume_decline, \n            price_change_1m, price_change_3m, sharpe_ratio\n        )\n        \n        # Overall risk rating\n        risk_rating = self.determine_risk_rating(risk_flags)\n        \n        return {\n            'symbol': asset_data['symbol'],\n            'sector': asset_data['sector'],\n            'current_price': asset_data['current_price'],\n            'market_cap': asset_data['market_cap'],\n            \n            # Risk metrics\n            'volatility': volatility,\n            'max_drawdown': max_drawdown,\n            'volume_decline': volume_decline,\n            'sharpe_ratio': sharpe_ratio,\n            'beta': beta,\n            'rsi': rsi,\n            \n            # Performance metrics\n            'price_change_1m': price_change_1m,\n            'price_change_3m': price_change_3m,\n            'price_change_6m': price_change_6m,\n            \n            # Risk assessment\n            'risk_flags': risk_flags,\n            'risk_rating': risk_rating,\n            'risk_score': len([f for f in risk_flags.values() if f])\n        }\n    \n    def calculate_risk_flags(self, volatility: float, max_drawdown: float, \n                           volume_decline: float, price_change_1m: float, \n                           price_change_3m: float, sharpe_ratio: float) -> Dict[str, bool]:\n        \"\"\"\n        Calculate individual risk flags based on thresholds\n        \n        Args:\n            volatility: Annualized volatility\n            max_drawdown: Maximum drawdown\n            volume_decline: Volume decline ratio\n            price_change_1m: 1-month price change\n            price_change_3m: 3-month price change\n            sharpe_ratio: Risk-adjusted return ratio\n            \n        Returns:\n            Dictionary of risk flags\n        \"\"\"\n        return {\n            'high_volatility': volatility > self.risk_thresholds['volatility_red'],\n            'extreme_drawdown': max_drawdown < self.risk_thresholds['drawdown_red'],\n            'volume_collapse': volume_decline < self.risk_thresholds['volume_decline_red'],\n            'severe_decline': price_change_1m < -0.15,  # -15% in 1 month\n            'extended_decline': price_change_3m < -0.25,  # -25% in 3 months\n            'poor_risk_return': sharpe_ratio < -0.5,\n            'momentum_breakdown': price_change_1m < -0.1 and price_change_3m < -0.1\n        }\n    \n    def determine_risk_rating(self, risk_flags: Dict[str, bool]) -> str:\n        \"\"\"\n        Determine overall risk rating based on flags\n        \n        Args:\n            risk_flags: Dictionary of risk flags\n            \n        Returns:\n            Risk rating: 'RED', 'YELLOW', or 'GREEN'\n        \"\"\"\n        critical_flags = ['extreme_drawdown', 'volume_collapse', 'severe_decline']\n        warning_flags = ['high_volatility', 'extended_decline', 'poor_risk_return']\n        \n        # RED rating if any critical flag is true\n        if any(risk_flags[flag] for flag in critical_flags):\n            return 'RED'\n        \n        # YELLOW rating if 2 or more warning flags\n        warning_count = sum(risk_flags[flag] for flag in warning_flags)\n        if warning_count >= 2:\n            return 'YELLOW'\n        \n        # Additional YELLOW conditions\n        if risk_flags['momentum_breakdown'] and warning_count >= 1:\n            return 'YELLOW'\n        \n        return 'GREEN'\n    \n    def calculate_rsi(self, prices: np.ndarray, period: int = 14) -> float:\n        \"\"\"\n        Calculate Relative Strength Index\n        \n        Args:\n            prices: Array of historical prices\n            period: RSI calculation period\n            \n        Returns:\n            RSI value\n        \"\"\"\n        if len(prices) < period + 1:\n            return 50.0  # Neutral RSI\n        \n        deltas = np.diff(prices)\n        gains = np.where(deltas > 0, deltas, 0)\n        losses = np.where(deltas < 0, -deltas, 0)\n        \n        avg_gain = np.mean(gains[-period:])\n        avg_loss = np.mean(losses[-period:])\n        \n        if avg_loss == 0:\n            return 100.0\n        \n        rs = avg_gain / avg_loss\n        rsi = 100 - (100 / (1 + rs))\n        \n        return rsi\n    \n    def calculate_beta(self, asset_returns: np.ndarray) -> float:\n        \"\"\"\n        Calculate beta relative to market (simulated market returns)\n        \n        Args:\n            asset_returns: Array of asset returns\n            \n        Returns:\n            Beta coefficient\n        \"\"\"\n        # Generate mock market returns for beta calculation\n        market_returns = np.random.normal(0.0008, 0.012, len(asset_returns))  # ~8% annual return, 12% vol\n        \n        if len(asset_returns) < 30:  # Need sufficient data points\n            return 1.0\n        \n        covariance = np.cov(asset_returns, market_returns)[0][1]\n        market_variance = np.var(market_returns)\n        \n        if market_variance == 0:\n            return 1.0\n        \n        beta = covariance / market_variance\n        return beta\n    \n    def add_correlation_analysis(self, analysis_results: List[Dict], portfolio_data: List[Dict]):\n        \"\"\"\n        Add correlation analysis between assets\n        \n        Args:\n            analysis_results: List of analysis results to update\n            portfolio_data: Original portfolio data\n        \"\"\"\n        # Build correlation matrix\n        price_data = {}\n        for asset in portfolio_data:\n            symbol = asset['symbol']\n            prices = np.array(asset['historical_prices'])\n            if len(prices) > 1:\n                returns = np.diff(prices) / prices[:-1]\n                price_data[symbol] = returns\n        \n        # Calculate pairwise correlations for each asset\n        for i, result in enumerate(analysis_results):\n            symbol = result['symbol']\n            if symbol in price_data:\n                correlations = []\n                for other_symbol, other_returns in price_data.items():\n                    if other_symbol != symbol:\n                        if len(price_data[symbol]) == len(other_returns):\n                            corr = np.corrcoef(price_data[symbol], other_returns)[0, 1]\n                            if not np.isnan(corr):\n                                correlations.append(abs(corr))\n                \n                if correlations:\n                    result['avg_correlation'] = np.mean(correlations)\n                    result['max_correlation'] = np.max(correlations)\n                    result['high_correlation_flag'] = result['max_correlation'] > self.risk_thresholds['correlation_threshold']\n                else:\n                    result['avg_correlation'] = 0.0\n                    result['max_correlation'] = 0.0\n                    result['high_correlation_flag'] = False\n","size_bytes":10645},"utils/mock_data.py":{"content":"import random\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Any\n\nclass MockBloombergData:\n    \"\"\"\n    Mock data generator to simulate Bloomberg data feeds\n    Generates realistic portfolio and market data for demonstration\n    \"\"\"\n    \n    def __init__(self):\n        self.sectors = [\n            'Technology', 'Healthcare', 'Financial Services', 'Consumer Discretionary',\n            'Industrial', 'Energy', 'Materials', 'Utilities', 'Real Estate', 'Consumer Staples'\n        ]\n        \n        self.company_prefixes = [\n            'Global', 'Advanced', 'United', 'American', 'International', 'Pacific',\n            'National', 'Premier', 'Superior', 'Dynamic', 'Strategic', 'Innovative',\n            'First', 'Capital', 'Metro', 'Regional', 'Consolidated', 'Alliance'\n        ]\n        \n        self.company_suffixes = [\n            'Corp', 'Inc', 'LLC', 'Group', 'Holdings', 'Systems', 'Technologies',\n            'Solutions', 'Industries', 'Enterprises', 'Partners', 'Capital',\n            'Resources', 'Services', 'International', 'Global'\n        ]\n        \n        self.symbol_prefixes = ['A', 'B', 'C', 'D', 'G', 'M', 'N', 'P', 'R', 'S', 'T', 'V', 'X', 'Z']\n    \n    def generate_asset_data(self) -> Dict[str, Any]:\n        \"\"\"\n        Generate mock data for a single asset\n        \n        Returns:\n            Dictionary containing asset information\n        \"\"\"\n        # Generate company name and symbol\n        prefix = random.choice(self.company_prefixes)\n        suffix = random.choice(self.company_suffixes)\n        company_name = f\"{prefix} {suffix}\"\n        \n        # Generate symbol (2-4 characters)\n        symbol_length = random.choice([2, 3, 3, 4])  # Bias toward 3-character symbols\n        symbol = random.choice(self.symbol_prefixes)\n        for _ in range(symbol_length - 1):\n            symbol += random.choice('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n        \n        # Sector and basic info\n        sector = random.choice(self.sectors)\n        \n        # Price data - realistic ranges based on sector\n        if sector in ['Technology', 'Healthcare']:\n            base_price = random.uniform(50, 400)\n            volatility_base = random.uniform(0.15, 0.45)\n        elif sector in ['Financial Services', 'Energy']:\n            base_price = random.uniform(30, 150)\n            volatility_base = random.uniform(0.20, 0.50)\n        elif sector in ['Utilities', 'Consumer Staples']:\n            base_price = random.uniform(40, 120)\n            volatility_base = random.uniform(0.10, 0.25)\n        else:\n            base_price = random.uniform(35, 200)\n            volatility_base = random.uniform(0.15, 0.35)\n        \n        # Market cap based on price and shares outstanding\n        shares_outstanding = random.randint(50_000_000, 2_000_000_000)\n        market_cap = base_price * shares_outstanding\n        \n        # Additional financial metrics\n        pe_ratio = random.uniform(8, 35) if random.random() > 0.1 else None  # 10% chance of no P/E\n        dividend_yield = random.uniform(0, 0.06) if random.random() > 0.3 else 0  # 70% pay dividends\n        \n        return {\n            'symbol': symbol,\n            'company_name': company_name,\n            'sector': sector,\n            'current_price': round(base_price, 2),\n            'market_cap': int(market_cap),\n            'shares_outstanding': shares_outstanding,\n            'pe_ratio': round(pe_ratio, 2) if pe_ratio else None,\n            'dividend_yield': round(dividend_yield, 4),\n            'volatility_base': volatility_base,  # Used for historical price generation\n            'currency': 'USD',\n            'exchange': random.choice(['NYSE', 'NASDAQ', 'NYSE', 'NASDAQ']),  # Bias toward major exchanges\n            'country': 'United States'\n        }\n    \n    def generate_historical_prices(self, current_price: float, days: int = 252) -> Dict[str, List]:\n        \"\"\"\n        Generate realistic historical price data using geometric Brownian motion\n        \n        Args:\n            current_price: Current/ending price\n            days: Number of historical days to generate\n            \n        Returns:\n            Dictionary with prices, dates, and volumes\n        \"\"\"\n        # Parameters for price simulation\n        dt = 1/252  # Daily time step\n        mu = random.uniform(-0.05, 0.15)  # Annual drift (return)\n        sigma = random.uniform(0.15, 0.45)  # Annual volatility\n        \n        # Generate price path working backwards from current price\n        prices = [current_price]\n        \n        for i in range(days - 1):\n            # Random walk component\n            dW = np.random.normal(0, np.sqrt(dt))\n            \n            # Geometric Brownian motion\n            prev_price = prices[-1]\n            new_price = prev_price * np.exp((mu - 0.5 * sigma**2) * dt + sigma * dW)\n            \n            # Add some mean reversion to keep prices reasonable\n            if new_price > current_price * 2:\n                new_price = current_price * 2 * random.uniform(0.9, 1.0)\n            elif new_price < current_price * 0.3:\n                new_price = current_price * 0.3 * random.uniform(1.0, 1.1)\n            \n            prices.append(max(0.01, new_price))  # Ensure positive prices\n        \n        # Reverse to get chronological order\n        prices.reverse()\n        \n        # Generate corresponding dates\n        end_date = datetime.now().date()\n        dates = []\n        for i in range(days):\n            date = end_date - timedelta(days=days-1-i)\n            dates.append(date.isoformat())\n        \n        # Generate trading volumes (correlated with price movements)\n        volumes = []\n        base_volume = random.randint(100_000, 10_000_000)\n        \n        for i in range(len(prices)):\n            # Volume tends to be higher on days with large price movements\n            if i > 0:\n                price_change = abs(prices[i] - prices[i-1]) / prices[i-1]\n                volume_multiplier = 1 + price_change * 3  # Higher volume on volatile days\n            else:\n                volume_multiplier = 1\n            \n            daily_volume = int(base_volume * volume_multiplier * random.uniform(0.5, 2.0))\n            volumes.append(daily_volume)\n        \n        return {\n            'prices': [round(p, 2) for p in prices],\n            'dates': dates,\n            'volumes': volumes\n        }\n    \n    def generate_market_indices(self, days: int = 252) -> Dict[str, Dict]:\n        \"\"\"\n        Generate mock market index data for correlation analysis\n        \n        Args:\n            days: Number of historical days\n            \n        Returns:\n            Dictionary with market index data\n        \"\"\"\n        indices = {\n            'S&P 500': {'current_level': 4200, 'volatility': 0.16},\n            'NASDAQ': {'current_level': 13000, 'volatility': 0.20},\n            'DOW JONES': {'current_level': 34000, 'volatility': 0.15}\n        }\n        \n        market_data = {}\n        \n        for index_name, params in indices.items():\n            historical_data = self.generate_historical_prices(\n                params['current_level'], days\n            )\n            market_data[index_name] = historical_data\n        \n        return market_data\n    \n    def generate_economic_indicators(self) -> Dict[str, Any]:\n        \"\"\"\n        Generate mock economic indicators\n        \n        Returns:\n            Dictionary with economic indicators\n        \"\"\"\n        return {\n            'interest_rates': {\n                'fed_funds_rate': round(random.uniform(0.25, 5.5), 2),\n                '10_year_treasury': round(random.uniform(1.5, 6.0), 2),\n                '30_year_treasury': round(random.uniform(2.0, 6.5), 2)\n            },\n            'economic_metrics': {\n                'gdp_growth': round(random.uniform(-2.0, 4.0), 1),\n                'unemployment_rate': round(random.uniform(3.5, 8.0), 1),\n                'inflation_rate': round(random.uniform(1.0, 6.0), 1),\n                'consumer_confidence': round(random.uniform(80, 140), 1)\n            },\n            'market_metrics': {\n                'vix': round(random.uniform(12, 35), 1),\n                'dollar_index': round(random.uniform(95, 110), 2),\n                'oil_price': round(random.uniform(60, 120), 2),\n                'gold_price': round(random.uniform(1800, 2100), 2)\n            }\n        }\n    \n    def generate_news_headlines(self, symbol: str, sentiment_bias: str = 'mixed') -> List[Dict[str, Any]]:\n        \"\"\"\n        Generate mock news headlines for sentiment analysis\n        \n        Args:\n            symbol: Asset symbol\n            sentiment_bias: 'positive', 'negative', or 'mixed'\n            \n        Returns:\n            List of news headline dictionaries\n        \"\"\"\n        positive_templates = [\n            f\"{symbol} reports strong quarterly earnings, beats estimates\",\n            f\"{symbol} announces major partnership with industry leader\",\n            f\"Analysts upgrade {symbol} target price following positive outlook\",\n            f\"{symbol} receives FDA approval for breakthrough product\",\n            f\"{symbol} CEO outlines ambitious growth strategy at investor day\"\n        ]\n        \n        negative_templates = [\n            f\"{symbol} misses quarterly expectations, guidance lowered\",\n            f\"Regulatory concerns weigh on {symbol} stock price\",\n            f\"{symbol} faces increased competition in core markets\",\n            f\"Credit rating agency places {symbol} on negative watch\",\n            f\"{symbol} announces restructuring, job cuts expected\"\n        ]\n        \n        neutral_templates = [\n            f\"{symbol} announces quarterly dividend payment\",\n            f\"{symbol} schedules earnings call for next week\",\n            f\"Trading volume in {symbol} remains elevated\",\n            f\"{symbol} stock included in new ESG index\",\n            f\"Analyst maintains neutral rating on {symbol} shares\"\n        ]\n        \n        headlines = []\n        num_headlines = random.randint(10, 25)\n        \n        for _ in range(num_headlines):\n            if sentiment_bias == 'positive':\n                template_choice = random.choices(\n                    [positive_templates, neutral_templates, negative_templates],\n                    weights=[0.6, 0.3, 0.1]\n                )[0]\n            elif sentiment_bias == 'negative':\n                template_choice = random.choices(\n                    [positive_templates, neutral_templates, negative_templates],\n                    weights=[0.1, 0.2, 0.7]\n                )[0]\n            else:  # mixed\n                template_choice = random.choices(\n                    [positive_templates, neutral_templates, negative_templates],\n                    weights=[0.3, 0.4, 0.3]\n                )[0]\n            \n            headline = random.choice(template_choice)\n            \n            # Generate publication date\n            days_ago = random.randint(1, 365)\n            pub_date = datetime.now() - timedelta(days=days_ago)\n            \n            headlines.append({\n                'headline': headline,\n                'publication_date': pub_date.isoformat(),\n                'source': random.choice([\n                    'Reuters', 'Bloomberg', 'MarketWatch', 'Yahoo Finance',\n                    'CNBC', 'Financial Times', 'Wall Street Journal'\n                ])\n            })\n        \n        return headlines\n","size_bytes":11379},"replit.md":{"content":"# Portfolio Risk Analysis Pipeline\n\n## Overview\nThis project is a multi-stage portfolio risk analysis application built with Streamlit. It simulates a Bloomberg data integration pipeline to analyze financial portfolios through five sequential stages: (1) Data Ingestion, (2) Core Risk Analysis, (3) ML Analysis (Anomaly Detection & Risk Prediction), (4) Sentiment Analysis for high-risk assets, and (5) Comprehensive PDF Report Generation. The application uses mock data to simulate real-world financial feeds, applying time-series analysis, rule-based risk scoring, advanced machine learning, and NLP-based sentiment analysis to provide actionable investment insights. The primary goal is to empower users with a powerful tool for understanding and mitigating portfolio risks.\n\n## User Preferences\nPreferred communication style: Simple, everyday language.\n\n## AI & Machine Learning Explained (For Beginners)\n\nThis application uses artificial intelligence (AI) and machine learning (ML) to help analyze investment portfolios. If you're new to these concepts, here's what they mean and how they work in this application:\n\n### What is AI and Machine Learning?\n\n**Artificial Intelligence (AI)** is like giving computers the ability to make smart decisions, similar to how humans think and learn. Instead of following strict rules, AI can recognize patterns and make predictions.\n\n**Machine Learning (ML)** is a type of AI where computers learn from examples. Think of it like teaching a child to recognize animals - you show them many pictures of cats and dogs, and eventually they learn to tell the difference on their own. ML works the same way with data.\n\n### The Three ML Techniques We Use\n\n#### 1. Anomaly Detection (Finding the Unusual)\n\n**What it does:** Identifies investments that are behaving strangely or differently from the rest of your portfolio.\n\n**How it works:** We use a technique called \"Isolation Forest\" - imagine you have a forest of trees, and you're trying to find which fruit is different from the others. The unusual fruits are easier to isolate (separate) from the group because they don't fit the normal pattern.\n\n**In practice:** The system looks at things like price swings, trading volume, and returns for all your investments. If one stock is behaving very differently - maybe dropping dramatically while others are stable, or showing unusually high volatility - it flags it as an \"anomaly\" (something unusual).\n\n**What you get:**\n- **Anomaly Score (0-100)**: Higher scores mean more unusual behavior\n- **Severity Level**: LOW, MEDIUM, HIGH, or CRITICAL\n- **Contributing Factors**: Which specific measurements made it unusual (e.g., extreme price drops, unusual trading volume)\n- **Recommendations**: What action to take based on the severity\n\n#### 2. Risk Prediction (Forecasting the Future)\n\n**What it does:** Predicts whether an investment's risk level will stay the same, get better, or get worse.\n\n**How it works:** We use \"Random Forest Classifier\" - imagine you're asking advice from 100 different financial experts. Each expert looks at the investment data and makes their own prediction. Then you take a vote: whatever most experts agree on becomes the final prediction. This \"wisdom of the crowd\" approach makes predictions more reliable.\n\n**In practice:** The system trains itself by looking at patterns in your current portfolio data. It learns which combinations of metrics (like volatility, price trends, and trading patterns) typically lead to high-risk vs. low-risk ratings. Then it applies this knowledge to predict future risk levels.\n\n**What you get:**\n- **Predicted Risk Rating**: GREEN (low risk), YELLOW (moderate), or RED (high risk)\n- **Confidence Score**: How sure the system is about its prediction (0-100%)\n- **Risk Trend**: Whether risk is IMPROVING, DETERIORATING, or STABLE\n- **Key Risk Drivers**: Which factors are most important in determining risk\n\n#### 3. Sentiment Analysis (Reading the News)\n\n**What it does:** Reads financial news headlines and determines if they're positive, negative, or neutral about your investments.\n\n**How it works:** We use \"TextBlob\" - a tool that understands language context. It's like having someone read all the news for you and tell you whether the overall tone is good news, bad news, or neutral.\n\n**In practice:** For high-risk (RED-flagged) investments, the system gathers recent news articles and analyzes the language used. Words like \"soaring,\" \"breakthrough,\" or \"record profits\" suggest positive sentiment. Words like \"plummeting,\" \"investigation,\" or \"losses\" suggest negative sentiment.\n\n**What you get:**\n- **Sentiment Score (-1 to +1)**: Negative to positive scale\n- **Sentiment Label**: NEGATIVE, NEUTRAL, or POSITIVE\n- **Sentiment Trend**: Whether news is getting better or worse over time\n- **Key Themes**: What topics are being discussed (earnings, regulatory issues, management changes, etc.)\n- **Confidence Score**: How reliable the sentiment analysis is (based on number of articles and consistency)\n\n### Why Use Machine Learning?\n\n1. **Pattern Recognition**: ML can spot complex patterns in large amounts of data that humans might miss.\n\n2. **Speed**: It can analyze thousands of data points across many investments in seconds.\n\n3. **Consistency**: Unlike humans, ML doesn't get tired or emotional - it applies the same analysis standards every time.\n\n4. **Early Warning System**: By detecting anomalies and predicting trends, ML can alert you to potential problems before they become serious.\n\n5. **Data-Driven Decisions**: Instead of relying on gut feelings, ML provides objective insights based on actual data patterns.\n\n### Important Limitations to Understand\n\n- **Historical Data**: ML learns from past patterns. It can't predict completely new situations or \"black swan\" events (unexpected market crashes).\n\n- **Not 100% Accurate**: The predictions are probabilities, not certainties. A 90% confidence score means there's still a 10% chance the prediction is wrong.\n\n- **Requires Good Data**: ML is only as good as the data it learns from. In this application, we use simulated (mock) data for demonstration purposes.\n\n- **Human Judgment Still Matters**: ML is a tool to help you make better decisions, not to make decisions for you. Always combine ML insights with your own research and judgment.\n\n### How to Interpret the Results\n\nWhen you see ML analysis results:\n\n1. **High anomaly scores (60+)** deserve immediate attention - investigate why the investment is unusual.\n\n2. **Risk predictions showing DETERIORATING trends** are early warnings to review those positions.\n\n3. **Negative sentiment with high confidence** suggests you should check what's happening with that company.\n\n4. **Look at the \"Key Drivers\" and \"Contributing Factors\"** - they tell you *why* the system flagged something, which is often more valuable than the flag itself.\n\nRemember: This application is designed for learning and demonstration. In real-world scenarios, always consult with financial professionals before making investment decisions.\n\n## System Architecture\n\n### Frontend Architecture\n- **Framework**: Streamlit web application with a single-page design and sidebar controls.\n- **State Management**: Streamlit session state for pipeline results and timings.\n- **UI/UX**: Interactive elements (sliders, buttons), wide layout for visualizations.\n- **Visualization**: Matplotlib for charts, ReportLab for PDF generation.\n- **Downloads**: PDF report, portfolio CSV, and risk analysis CSV download options.\n\n### Backend Architecture\n- **Pipeline Pattern**: A five-stage sequential processing pipeline:\n  1. **Stage 1 - Data Ingestion**: Simulates Bloomberg API data fetching.\n  2. **Stage 2 - Core Analysis**: Performs time-series and rule-based risk scoring.\n  3. **Stage 3 - ML Analysis**: Conducts anomaly detection (Isolation Forest) and risk prediction (Random Forest Classifier).\n  4. **Stage 4 - Sentiment Analysis**: NLP-based analysis exclusively on RED-flagged assets.\n  5. **Stage 5 - Report Generation**: Creates PDF reports with visualizations and ML insights.\n- **Modular Design**: Each pipeline stage is encapsulated in its own engine class.\n- **Data Flow**: Linear progression with staged transformations.\n- **Risk Score Calculation**: Based on 7 distinct risk flags (e.g., high_volatility, extreme_drawdown).\n- **Risk Rating Classification**: Assets categorized as RED (High), YELLOW (Moderate), or GREEN (Low) based on hierarchical flag system.\n- **Sentiment Analysis**: Calculates weighted average sentiment, classifies sentiment (NEGATIVE, NEUTRAL, POSITIVE), analyzes trends, and provides a confidence score. Applied selectively to RED-flagged assets.\n- **Machine Learning**:\n    - **Anomaly Detection**: Isolation Forest identifies unusual behaviors, providing a score, severity (LOW, MEDIUM, HIGH, CRITICAL), and contributing factors.\n    - **Risk Rating Prediction**: Random Forest Classifier predicts future risk ratings (GREEN, YELLOW, RED), identifies key drivers, provides prediction confidence, and analyzes risk trends (IMPROVING, DETERIORATING, STABLE).\n- **ML Validation System**: Automated checks validate anomaly detection, risk prediction, feature quality, and feature importance.\n\n### Data Storage Solutions\n- **In-Memory**: Pandas DataFrames for all data manipulation.\n- **File System**: Reports saved to `/reports`, charts to `/charts`.\n- **Session State**: Streamlit session state for managing pipeline results.\n- **No Database**: Application is stateless.\n\n### Key Technologies\n- **Data Processing**: NumPy, Pandas\n- **Machine Learning**: Scikit-learn (Isolation Forest, Random Forest Classifier)\n- **NLP**: TextBlob\n- **Visualization**: Matplotlib\n- **PDF Generation**: ReportLab\n- **Web Framework**: Streamlit\n\n## External Dependencies\n\n### Third-party Libraries\n- **streamlit**: Web application framework.\n- **pandas**: Data manipulation.\n- **numpy**: Numerical operations.\n- **scikit-learn**: Machine learning algorithms.\n- **matplotlib**: Charting.\n- **reportlab**: PDF generation.\n- **textblob**: Sentiment analysis.\n\n### Simulated External Services\n- **Bloomberg API**: Simulated via `MockBloombergData` class for financial data (asset pricing, historical series, volume data, market capitalization, Bloomberg IDs).\n\n### Data Sources (Simulated)\n- **News Sources**: Mock news feeds from various financial outlets (Reuters, Bloomberg News, Financial Times, WSJ, etc.).\n- **Market Data**: Mock time-series data for prices, volumes, and technical indicators.\n\n### File System Dependencies\n- Requires write access to `/reports` and `/charts` directories.","size_bytes":10605},"pipeline/data_ingestion.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport random\nfrom typing import List, Dict, Any\nfrom utils.mock_data import MockBloombergData\n\nclass DataIngestionEngine:\n    \"\"\"\n    Stage 1: Data Ingestion Engine\n    Simulates Bloomberg data ingestion with realistic portfolio data\n    \"\"\"\n    \n    def __init__(self):\n        self.mock_data_generator = MockBloombergData()\n        self.connection_status = \"Connected\"\n    \n    def ingest_portfolio_data(self, portfolio_size: int = 25) -> List[Dict[str, Any]]:\n        \"\"\"\n        Ingest portfolio data from Bloomberg API (simulated)\n        \n        Args:\n            portfolio_size: Number of assets in portfolio\n            \n        Returns:\n            List of dictionaries containing asset data\n        \"\"\"\n        print(f\"Connecting to Bloomberg API...\")\n        print(f\"Fetching data for {portfolio_size} assets...\")\n        \n        # Generate mock portfolio data\n        portfolio_data = []\n        \n        for i in range(portfolio_size):\n            asset_data = self.mock_data_generator.generate_asset_data()\n            \n            # Add historical price data (252 trading days = 1 year)\n            historical_data = self.mock_data_generator.generate_historical_prices(\n                asset_data['current_price'], \n                days=252\n            )\n            \n            asset_data.update({\n                'data_ingestion_timestamp': datetime.now().isoformat(),\n                'historical_prices': historical_data['prices'],\n                'historical_dates': historical_data['dates'],\n                'trading_volume_history': historical_data['volumes'],\n                'bloomberg_id': f\"BBG{random.randint(100000000, 999999999)}\",\n                'data_quality_score': random.uniform(0.85, 1.0)\n            })\n            \n            portfolio_data.append(asset_data)\n        \n        print(f\"Successfully ingested data for {len(portfolio_data)} assets\")\n        return portfolio_data\n    \n    def validate_data_integrity(self, portfolio_data: List[Dict]) -> Dict[str, Any]:\n        \"\"\"\n        Validate the integrity of ingested data\n        \n        Args:\n            portfolio_data: List of asset data dictionaries\n            \n        Returns:\n            Validation results\n        \"\"\"\n        validation_results = {\n            'total_assets': len(portfolio_data),\n            'complete_records': 0,\n            'missing_data_assets': [],\n            'data_quality_issues': [],\n            'average_data_quality': 0.0\n        }\n        \n        quality_scores = []\n        \n        for asset in portfolio_data:\n            # Check for required fields\n            required_fields = ['symbol', 'current_price', 'historical_prices', 'market_cap']\n            missing_fields = [field for field in required_fields if field not in asset or asset[field] is None]\n            \n            if not missing_fields:\n                validation_results['complete_records'] += 1\n            else:\n                validation_results['missing_data_assets'].append({\n                    'symbol': asset.get('symbol', 'Unknown'),\n                    'missing_fields': missing_fields\n                })\n            \n            # Track data quality scores\n            if 'data_quality_score' in asset:\n                quality_scores.append(asset['data_quality_score'])\n                \n                if asset['data_quality_score'] < 0.9:\n                    validation_results['data_quality_issues'].append({\n                        'symbol': asset['symbol'],\n                        'quality_score': asset['data_quality_score']\n                    })\n        \n        if quality_scores:\n            validation_results['average_data_quality'] = np.mean(quality_scores)\n        \n        return validation_results\n    \n    def get_real_time_data(self, symbols: List[str]) -> Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Fetch real-time data for specific symbols (simulated)\n        \n        Args:\n            symbols: List of asset symbols\n            \n        Returns:\n            Dictionary with real-time data for each symbol\n        \"\"\"\n        real_time_data = {}\n        \n        for symbol in symbols:\n            real_time_data[symbol] = {\n                'last_price': random.uniform(50, 500),\n                'bid': random.uniform(50, 500),\n                'ask': random.uniform(50, 500),\n                'volume': random.randint(10000, 1000000),\n                'timestamp': datetime.now().isoformat(),\n                'change_percent': random.uniform(-5.0, 5.0)\n            }\n        \n        return real_time_data\n    \n    def check_connection_status(self) -> Dict[str, Any]:\n        \"\"\"\n        Check Bloomberg API connection status\n        \n        Returns:\n            Connection status information\n        \"\"\"\n        return {\n            'status': self.connection_status,\n            'last_check': datetime.now().isoformat(),\n            'latency_ms': random.uniform(50, 200),\n            'api_rate_limit_remaining': random.randint(800, 1000)\n        }\n","size_bytes":5073},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"matplotlib>=3.10.7\",\n    \"numpy>=2.3.3\",\n    \"pandas>=2.3.3\",\n    \"reportlab>=4.4.4\",\n    \"scikit-learn>=1.7.2\",\n    \"streamlit>=1.50.0\",\n    \"textblob>=0.19.0\",\n]\n","size_bytes":311},"pipeline/sentiment_analysis.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport random\nfrom typing import List, Dict, Any\nfrom textblob import TextBlob\nimport re\n\nclass SentimentAnalysisEngine:\n    \"\"\"\n    Stage 3: Sentiment Analysis Engine\n    Analyzes news sentiment for RED-flagged assets only\n    \"\"\"\n    \n    def __init__(self):\n        self.sentiment_threshold_negative = -0.3\n        self.sentiment_threshold_positive = 0.3\n        self.news_sources = [\n            \"Reuters\", \"Bloomberg News\", \"Financial Times\", \"Wall Street Journal\", \n            \"MarketWatch\", \"Yahoo Finance\", \"CNBC\", \"Seeking Alpha\"\n        ]\n    \n    def analyze_sentiment(self, red_flagged_assets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Analyze sentiment for RED-flagged assets only\n        \n        Args:\n            red_flagged_assets: List of RED-flagged assets from core analysis\n            \n        Returns:\n            List of sentiment analysis results\n        \"\"\"\n        sentiment_results = []\n        \n        for asset in red_flagged_assets:\n            # Fetch news for this asset (simulated)\n            news_articles = self.fetch_news_for_asset(asset['symbol'])\n            \n            # Perform sentiment analysis\n            asset_sentiment = self.analyze_asset_sentiment(asset, news_articles)\n            sentiment_results.append(asset_sentiment)\n        \n        return sentiment_results\n    \n    def fetch_news_for_asset(self, symbol: str, days_back: int = 365) -> List[Dict[str, Any]]:\n        \"\"\"\n        Fetch news articles for a specific asset (simulated)\n        \n        Args:\n            symbol: Asset symbol\n            days_back: Number of days to look back for news\n            \n        Returns:\n            List of news articles\n        \"\"\"\n        # Generate mock news articles with realistic financial content\n        news_templates = [\n            f\"{symbol} reports quarterly earnings miss, revenue down {random.randint(5, 25)}%\",\n            f\"Analyst downgrades {symbol} citing regulatory concerns and market headwinds\",\n            f\"{symbol} faces investigation over accounting practices, shares tumble\",\n            f\"CEO of {symbol} resigns amid strategic disagreements with board\",\n            f\"{symbol} announces major restructuring, plans to cut {random.randint(1000, 5000)} jobs\",\n            f\"Credit rating agency downgrades {symbol} debt to junk status\",\n            f\"{symbol} misses guidance for third consecutive quarter\",\n            f\"Regulatory approval delayed for {symbol}'s key product launch\",\n            f\"{symbol} competitor gains market share, pressure on margins continues\",\n            f\"Institutional investors reduce {symbol} holdings amid volatility concerns\",\n            f\"{symbol} explores strategic alternatives including potential sale\",\n            f\"Supply chain disruptions impact {symbol} production forecasts\",\n            f\"{symbol} settles lawsuit for ${random.randint(50, 500)} million\",\n            f\"Moody's places {symbol} on review for possible downgrade\",\n            f\"{symbol} withdraws full-year guidance citing economic uncertainty\"\n        ]\n        \n        positive_templates = [\n            f\"{symbol} beats earnings expectations, raises full-year guidance\",\n            f\"New partnership announced between {symbol} and major tech company\",\n            f\"{symbol} receives FDA approval for breakthrough therapy\",\n            f\"Activist investor takes stake in {symbol}, pushes for changes\",\n            f\"{symbol} announces share buyback program worth ${random.randint(100, 1000)}M\"\n        ]\n        \n        # Generate articles with bias toward negative news for RED-flagged assets\n        news_articles = []\n        num_articles = random.randint(15, 30)\n        \n        for i in range(num_articles):\n            # 70% negative, 20% neutral, 10% positive for RED-flagged assets\n            sentiment_bias = random.random()\n            \n            if sentiment_bias < 0.7:  # Negative news\n                headline = random.choice(news_templates)\n                sentiment_score = random.uniform(-0.8, -0.2)\n            elif sentiment_bias < 0.9:  # Neutral news\n                headline = f\"{symbol} trading volume increases amid sector rotation\"\n                sentiment_score = random.uniform(-0.1, 0.1)\n            else:  # Positive news\n                headline = random.choice(positive_templates)\n                sentiment_score = random.uniform(0.2, 0.6)\n            \n            # Generate article date\n            days_ago = random.randint(1, days_back)\n            article_date = datetime.now() - timedelta(days=days_ago)\n            \n            news_articles.append({\n                'headline': headline,\n                'source': random.choice(self.news_sources),\n                'published_date': article_date.isoformat(),\n                'url': f\"https://example-news.com/{symbol.lower()}-{random.randint(1000, 9999)}\",\n                'sentiment_score': sentiment_score,\n                'relevance_score': random.uniform(0.6, 1.0)\n            })\n        \n        return sorted(news_articles, key=lambda x: x['published_date'], reverse=True)\n    \n    def analyze_asset_sentiment(self, asset: Dict[str, Any], news_articles: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Perform sentiment analysis for a specific asset\n        \n        Args:\n            asset: Asset information\n            news_articles: List of news articles for the asset\n            \n        Returns:\n            Comprehensive sentiment analysis results\n        \"\"\"\n        if not news_articles:\n            return {\n                'symbol': asset['symbol'],\n                'news_count': 0,\n                'sentiment_score': 0.0,\n                'sentiment_label': 'NEUTRAL',\n                'confidence': 0.0,\n                'recent_news': [],\n                'sentiment_trend': 'STABLE'\n            }\n        \n        # Calculate overall sentiment metrics\n        sentiment_scores = [article['sentiment_score'] for article in news_articles]\n        relevance_scores = [article['relevance_score'] for article in news_articles]\n        \n        # Weighted average sentiment (by relevance)\n        weighted_sentiment = np.average(sentiment_scores, weights=relevance_scores)\n        \n        # Recent trend analysis (last 30 days vs. older news)\n        recent_cutoff = datetime.now() - timedelta(days=30)\n        recent_articles = [a for a in news_articles if datetime.fromisoformat(a['published_date'].replace('Z', '+00:00')).replace(tzinfo=None) > recent_cutoff]\n        older_articles = [a for a in news_articles if datetime.fromisoformat(a['published_date'].replace('Z', '+00:00')).replace(tzinfo=None) <= recent_cutoff]\n        \n        recent_sentiment = np.mean([a['sentiment_score'] for a in recent_articles]) if recent_articles else 0\n        older_sentiment = np.mean([a['sentiment_score'] for a in older_articles]) if older_articles else 0\n        \n        # Determine sentiment trend\n        if recent_sentiment > older_sentiment + 0.1:\n            sentiment_trend = 'IMPROVING'\n        elif recent_sentiment < older_sentiment - 0.1:\n            sentiment_trend = 'DETERIORATING'\n        else:\n            sentiment_trend = 'STABLE'\n        \n        # Sentiment label\n        if weighted_sentiment <= self.sentiment_threshold_negative:\n            sentiment_label = 'NEGATIVE'\n        elif weighted_sentiment >= self.sentiment_threshold_positive:\n            sentiment_label = 'POSITIVE'\n        else:\n            sentiment_label = 'NEUTRAL'\n        \n        # Calculate confidence based on number of articles and consistency\n        sentiment_std = np.std(sentiment_scores)\n        confidence = min(1.0, len(news_articles) / 20.0) * (1.0 - min(1.0, sentiment_std))\n        \n        # Key themes extraction (simulated)\n        key_themes = self.extract_key_themes(news_articles)\n        \n        # Recent significant news\n        recent_significant = sorted(\n            [a for a in recent_articles if abs(a['sentiment_score']) > 0.4],\n            key=lambda x: abs(x['sentiment_score']),\n            reverse=True\n        )[:5]\n        \n        return {\n            'symbol': asset['symbol'],\n            'sector': asset['sector'],\n            'risk_rating': asset['risk_rating'],\n            \n            # Sentiment metrics\n            'sentiment_score': weighted_sentiment,\n            'sentiment_label': sentiment_label,\n            'confidence': confidence,\n            'sentiment_trend': sentiment_trend,\n            \n            # News analysis\n            'news_count': len(news_articles),\n            'recent_news_count': len(recent_articles),\n            'negative_news_ratio': len([a for a in news_articles if a['sentiment_score'] < -0.2]) / len(news_articles),\n            \n            # Key insights\n            'key_themes': key_themes,\n            'recent_significant_news': recent_significant,\n            \n            # Time-based analysis\n            'recent_sentiment': recent_sentiment,\n            'older_sentiment': older_sentiment,\n            'sentiment_volatility': sentiment_std,\n            \n            # Analysis timestamp\n            'analysis_timestamp': datetime.now().isoformat()\n        }\n    \n    def extract_key_themes(self, news_articles: List[Dict[str, Any]]) -> List[str]:\n        \"\"\"\n        Extract key themes from news headlines (simplified approach)\n        \n        Args:\n            news_articles: List of news articles\n            \n        Returns:\n            List of key themes\n        \"\"\"\n        # Common financial themes/keywords\n        theme_keywords = {\n            'earnings': ['earnings', 'revenue', 'profit', 'loss', 'guidance'],\n            'regulatory': ['investigation', 'regulatory', 'compliance', 'lawsuit', 'settlement'],\n            'management': ['CEO', 'resign', 'leadership', 'board', 'management'],\n            'market_share': ['competitor', 'market share', 'competitive'],\n            'financial_health': ['debt', 'credit', 'rating', 'downgrade', 'bankruptcy'],\n            'operations': ['restructuring', 'layoffs', 'production', 'supply chain'],\n            'growth': ['expansion', 'partnership', 'acquisition', 'buyback', 'investment']\n        }\n        \n        # Count theme occurrences\n        theme_counts = {theme: 0 for theme in theme_keywords}\n        \n        for article in news_articles:\n            headline_lower = article['headline'].lower()\n            for theme, keywords in theme_keywords.items():\n                if any(keyword in headline_lower for keyword in keywords):\n                    theme_counts[theme] += 1\n        \n        # Return top themes\n        sorted_themes = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True)\n        return [theme for theme, count in sorted_themes if count > 0][:5]\n    \n    def calculate_sentiment_impact_score(self, sentiment_result: Dict[str, Any], \n                                       risk_metrics: Dict[str, Any]) -> float:\n        \"\"\"\n        Calculate combined sentiment-risk impact score\n        \n        Args:\n            sentiment_result: Sentiment analysis results\n            risk_metrics: Risk metrics from core analysis\n            \n        Returns:\n            Combined impact score (0-1, higher = more concerning)\n        \"\"\"\n        # Base sentiment impact\n        sentiment_impact = abs(sentiment_result['sentiment_score']) * sentiment_result['confidence']\n        \n        # Adjust for trend\n        if sentiment_result['sentiment_trend'] == 'DETERIORATING':\n            sentiment_impact *= 1.2\n        elif sentiment_result['sentiment_trend'] == 'IMPROVING':\n            sentiment_impact *= 0.8\n        \n        # Combine with risk metrics\n        risk_score = risk_metrics.get('risk_score', 0) / 7.0  # Normalize to 0-1\n        volatility_score = min(1.0, risk_metrics.get('volatility', 0) / 0.5)  # Cap at 50% vol\n        \n        # Weighted combination\n        combined_score = (\n            0.4 * sentiment_impact +\n            0.3 * risk_score +\n            0.3 * volatility_score\n        )\n        \n        return min(1.0, combined_score)\n","size_bytes":12137},"pipeline/ml_analysis.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Tuple\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass MLAnalysisEngine:\n    \"\"\"\n    Machine Learning Analysis Engine\n    Performs anomaly detection and risk prediction on portfolio assets\n    \"\"\"\n    \n    def __init__(self):\n        self.scaler = StandardScaler()\n        self.anomaly_detector = None\n        self.risk_predictor = None\n        self.feature_columns = [\n            'volatility', 'max_drawdown', 'volume_decline', 'sharpe_ratio', \n            'beta', 'rsi', 'price_change_1m', 'price_change_3m', 'price_change_6m'\n        ]\n    \n    def analyze_portfolio_ml(self, analysis_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Perform comprehensive ML analysis on portfolio\n        \n        Args:\n            analysis_results: Results from core analysis engine\n            \n        Returns:\n            Dictionary containing ML analysis results\n        \"\"\"\n        # Prepare feature matrix\n        features_df = self._prepare_features(analysis_results)\n        \n        # Perform anomaly detection\n        anomaly_results = self._detect_anomalies(features_df, analysis_results)\n        \n        # Perform risk prediction\n        risk_prediction_results = self._predict_risk_ratings(features_df, analysis_results)\n        \n        # Calculate feature importance\n        feature_importance = self._calculate_feature_importance(features_df, analysis_results)\n        \n        # Generate ML insights summary\n        ml_summary = self._generate_ml_summary(\n            anomaly_results, risk_prediction_results, feature_importance\n        )\n        \n        # Validate ML results\n        validation_results = self._validate_ml_results(\n            anomaly_results, risk_prediction_results, feature_importance, features_df\n        )\n        \n        return {\n            'anomaly_detection': anomaly_results,\n            'risk_prediction': risk_prediction_results,\n            'feature_importance': feature_importance,\n            'ml_summary': ml_summary,\n            'validation': validation_results,\n            'analysis_timestamp': datetime.now().isoformat()\n        }\n    \n    def _prepare_features(self, analysis_results: List[Dict[str, Any]]) -> pd.DataFrame:\n        \"\"\"\n        Prepare feature matrix for ML analysis\n        \n        Args:\n            analysis_results: Analysis results from core engine\n            \n        Returns:\n            DataFrame with features for ML\n        \"\"\"\n        features_data = []\n        \n        for asset in analysis_results:\n            feature_row = {\n                'symbol': asset['symbol'],\n                'sector': asset['sector'],\n                'current_price': asset['current_price'],\n                'market_cap': asset['market_cap'],\n                'risk_rating': asset['risk_rating']\n            }\n            \n            # Add numerical features\n            for feature in self.feature_columns:\n                feature_row[feature] = asset.get(feature, 0)\n            \n            # Add correlation features if available\n            feature_row['avg_correlation'] = asset.get('avg_correlation', 0)\n            feature_row['max_correlation'] = asset.get('max_correlation', 0)\n            \n            features_data.append(feature_row)\n        \n        return pd.DataFrame(features_data)\n    \n    def _detect_anomalies(self, features_df: pd.DataFrame, \n                         analysis_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Detect anomalies using Isolation Forest\n        \n        Args:\n            features_df: Feature DataFrame\n            analysis_results: Original analysis results\n            \n        Returns:\n            List of anomaly detection results\n        \"\"\"\n        # Select features for anomaly detection\n        X = features_df[self.feature_columns].fillna(0)\n        \n        # Normalize features\n        X_scaled = self.scaler.fit_transform(X)\n        \n        # Train Isolation Forest\n        self.anomaly_detector = IsolationForest(\n            contamination=0.15,  # Expect ~15% anomalies\n            random_state=42,\n            n_estimators=100\n        )\n        \n        # Predict anomalies (-1 for anomaly, 1 for normal)\n        anomaly_labels = self.anomaly_detector.fit_predict(X_scaled)\n        \n        # Get anomaly scores (lower score = more anomalous)\n        anomaly_scores = self.anomaly_detector.score_samples(X_scaled)\n        \n        # Normalize scores to 0-100 scale (higher = more anomalous)\n        min_score = anomaly_scores.min()\n        max_score = anomaly_scores.max()\n        \n        # Guard against division by zero when all scores are identical\n        if max_score == min_score:\n            # All assets have same anomaly score - treat as low anomaly\n            normalized_scores = np.full(len(anomaly_scores), 30.0)\n        else:\n            normalized_scores = 100 * (1 - (anomaly_scores - min_score) / (max_score - min_score))\n        \n        # Compile results\n        anomaly_results = []\n        for i, asset in enumerate(analysis_results):\n            is_anomaly = anomaly_labels[i] == -1\n            anomaly_score = normalized_scores[i]\n            \n            # Determine anomaly severity\n            if anomaly_score >= 80:\n                severity = 'CRITICAL'\n            elif anomaly_score >= 60:\n                severity = 'HIGH'\n            elif anomaly_score >= 40:\n                severity = 'MEDIUM'\n            else:\n                severity = 'LOW'\n            \n            # Identify which features contribute most to anomaly\n            contributing_features = self._identify_anomalous_features(\n                features_df.iloc[i], features_df\n            )\n            \n            anomaly_results.append({\n                'symbol': asset['symbol'],\n                'sector': asset['sector'],\n                'is_anomaly': is_anomaly,\n                'anomaly_score': round(anomaly_score, 2),\n                'severity': severity,\n                'risk_rating': asset['risk_rating'],\n                'contributing_features': contributing_features,\n                'recommendation': self._get_anomaly_recommendation(is_anomaly, anomaly_score)\n            })\n        \n        return sorted(anomaly_results, key=lambda x: x['anomaly_score'], reverse=True)\n    \n    def _identify_anomalous_features(self, asset_features: pd.Series, \n                                    all_features: pd.DataFrame) -> List[str]:\n        \"\"\"\n        Identify which features make an asset anomalous\n        \n        Args:\n            asset_features: Features for single asset\n            all_features: Features for all assets\n            \n        Returns:\n            List of anomalous feature names\n        \"\"\"\n        anomalous_features = []\n        \n        for feature in self.feature_columns:\n            asset_value = asset_features[feature]\n            mean_value = all_features[feature].mean()\n            std_value = all_features[feature].std()\n            \n            # Check if value is more than 2 standard deviations from mean\n            if std_value > 0:\n                z_score = abs((asset_value - mean_value) / std_value)\n                if z_score > 2:\n                    anomalous_features.append(feature)\n        \n        return anomalous_features[:3]  # Return top 3\n    \n    def _predict_risk_ratings(self, features_df: pd.DataFrame, \n                             analysis_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Predict future risk ratings using ML classifier\n        \n        Args:\n            features_df: Feature DataFrame\n            analysis_results: Original analysis results\n            \n        Returns:\n            Risk prediction results\n        \"\"\"\n        # Prepare training data\n        X = features_df[self.feature_columns].fillna(0)\n        y = features_df['risk_rating'].map({'GREEN': 0, 'YELLOW': 1, 'RED': 2})\n        \n        # Check if we have enough data and diversity\n        if len(X) < 10 or len(y.unique()) < 2:\n            return {\n                'model_trained': False,\n                'predictions': [],\n                'accuracy': 0.0,\n                'message': 'Insufficient data or risk diversity for prediction model'\n            }\n        \n        # Split data for validation\n        if len(X) >= 15:\n            X_train, X_test, y_train, y_test = train_test_split(\n                X, y, test_size=0.3, random_state=42, stratify=y if len(y.unique()) > 1 else None\n            )\n        else:\n            X_train, X_test, y_train, y_test = X, X, y, y\n        \n        # Train Random Forest Classifier\n        self.risk_predictor = RandomForestClassifier(\n            n_estimators=100,\n            max_depth=10,\n            random_state=42,\n            class_weight='balanced'\n        )\n        \n        self.risk_predictor.fit(X_train, y_train)\n        \n        # Get the actual classes the model learned\n        actual_classes = self.risk_predictor.classes_\n        reverse_label_map = {0: 'GREEN', 1: 'YELLOW', 2: 'RED'}\n        class_to_label = {i: reverse_label_map[actual_classes[i]] for i in range(len(actual_classes))}\n        \n        # Calculate accuracy\n        train_accuracy = self.risk_predictor.score(X_train, y_train)\n        test_accuracy = self.risk_predictor.score(X_test, y_test) if len(X) >= 15 else train_accuracy\n        \n        # Make predictions with probability\n        predictions = self.risk_predictor.predict(X)\n        prediction_proba = self.risk_predictor.predict_proba(X)\n        \n        # Compile prediction results\n        prediction_results = []\n        \n        for i, asset in enumerate(analysis_results):\n            # Map prediction index to actual class label\n            predicted_class_idx = np.where(actual_classes == predictions[i])[0][0]\n            predicted_rating = class_to_label[predicted_class_idx]\n            actual_rating = asset['risk_rating']\n            \n            # Get confidence (probability of predicted class)\n            confidence = prediction_proba[i][predicted_class_idx] * 100\n            \n            # Check if prediction differs from current rating\n            rating_change = predicted_rating != actual_rating\n            \n            # Determine if trend is improving or deteriorating\n            rating_order = {'GREEN': 0, 'YELLOW': 1, 'RED': 2}\n            if rating_change:\n                if rating_order[predicted_rating] > rating_order[actual_rating]:\n                    trend = 'DETERIORATING'\n                else:\n                    trend = 'IMPROVING'\n            else:\n                trend = 'STABLE'\n            \n            # Build risk probabilities dict based on actual classes present\n            risk_probabilities = {}\n            for class_idx, class_label in class_to_label.items():\n                risk_probabilities[class_label] = round(prediction_proba[i][class_idx] * 100, 1)\n            \n            # Fill in missing classes with 0\n            for label in ['GREEN', 'YELLOW', 'RED']:\n                if label not in risk_probabilities:\n                    risk_probabilities[label] = 0.0\n            \n            prediction_results.append({\n                'symbol': asset['symbol'],\n                'sector': asset['sector'],\n                'current_rating': actual_rating,\n                'predicted_rating': predicted_rating,\n                'confidence': round(confidence, 1),\n                'rating_change': rating_change,\n                'trend': trend,\n                'risk_probabilities': risk_probabilities\n            })\n        \n        return {\n            'model_trained': True,\n            'predictions': prediction_results,\n            'train_accuracy': round(train_accuracy * 100, 1),\n            'test_accuracy': round(test_accuracy * 100, 1),\n            'total_assets_analyzed': len(prediction_results),\n            'rating_changes_predicted': sum(1 for p in prediction_results if p['rating_change'])\n        }\n    \n    def _calculate_feature_importance(self, features_df: pd.DataFrame, \n                                     analysis_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Calculate feature importance from risk prediction model\n        \n        Args:\n            features_df: Feature DataFrame\n            analysis_results: Original analysis results\n            \n        Returns:\n            List of feature importance scores\n        \"\"\"\n        if self.risk_predictor is None:\n            return []\n        \n        # Get feature importances\n        importances = self.risk_predictor.feature_importances_\n        \n        # Create feature importance list\n        feature_importance = []\n        for feature, importance in zip(self.feature_columns, importances):\n            feature_importance.append({\n                'feature': feature.replace('_', ' ').title(),\n                'importance': round(importance * 100, 2),\n                'rank': 0  # Will be set after sorting\n            })\n        \n        # Sort by importance and assign ranks\n        feature_importance.sort(key=lambda x: x['importance'], reverse=True)\n        for i, feature in enumerate(feature_importance):\n            feature['rank'] = i + 1\n        \n        return feature_importance\n    \n    def _get_anomaly_recommendation(self, is_anomaly: bool, anomaly_score: float) -> str:\n        \"\"\"\n        Generate recommendation based on anomaly detection\n        \n        Args:\n            is_anomaly: Whether asset is flagged as anomaly\n            anomaly_score: Anomaly score (0-100)\n            \n        Returns:\n            Recommendation string\n        \"\"\"\n        if not is_anomaly or anomaly_score < 40:\n            return \"Normal behavior pattern - Continue monitoring\"\n        elif anomaly_score < 60:\n            return \"Moderate anomaly detected - Review underlying fundamentals\"\n        elif anomaly_score < 80:\n            return \"Significant anomaly - Conduct thorough due diligence\"\n        else:\n            return \"Critical anomaly - Consider immediate position review\"\n    \n    def _generate_ml_summary(self, anomaly_results: List[Dict], \n                           risk_predictions: Dict, \n                           feature_importance: List[Dict]) -> Dict[str, Any]:\n        \"\"\"\n        Generate summary of ML analysis\n        \n        Args:\n            anomaly_results: Anomaly detection results\n            risk_predictions: Risk prediction results\n            feature_importance: Feature importance scores\n            \n        Returns:\n            ML summary dictionary\n        \"\"\"\n        # Anomaly summary\n        total_anomalies = sum(1 for a in anomaly_results if a['is_anomaly'])\n        critical_anomalies = sum(1 for a in anomaly_results if a['severity'] == 'CRITICAL')\n        high_anomalies = sum(1 for a in anomaly_results if a['severity'] == 'HIGH')\n        \n        # Risk prediction summary\n        rating_changes = 0\n        deteriorating_count = 0\n        improving_count = 0\n        \n        if risk_predictions.get('model_trained'):\n            predictions = risk_predictions['predictions']\n            rating_changes = sum(1 for p in predictions if p['rating_change'])\n            deteriorating_count = sum(1 for p in predictions if p['trend'] == 'DETERIORATING')\n            improving_count = sum(1 for p in predictions if p['trend'] == 'IMPROVING')\n        \n        # Top risk factors\n        top_risk_factors = [f['feature'] for f in feature_importance[:3]] if feature_importance else []\n        \n        return {\n            'total_assets_analyzed': len(anomaly_results),\n            'anomaly_summary': {\n                'total_anomalies': total_anomalies,\n                'critical_anomalies': critical_anomalies,\n                'high_anomalies': high_anomalies,\n                'anomaly_rate': round(total_anomalies / len(anomaly_results) * 100, 1) if anomaly_results else 0\n            },\n            'prediction_summary': {\n                'model_trained': risk_predictions.get('model_trained', False),\n                'rating_changes_predicted': rating_changes,\n                'deteriorating_assets': deteriorating_count,\n                'improving_assets': improving_count,\n                'model_accuracy': risk_predictions.get('test_accuracy', 0)\n            },\n            'top_risk_factors': top_risk_factors,\n            'key_insights': self._generate_key_insights(\n                total_anomalies, critical_anomalies, deteriorating_count, top_risk_factors\n            )\n        }\n    \n    def _generate_key_insights(self, total_anomalies: int, critical_anomalies: int,\n                               deteriorating_count: int, top_factors: List[str]) -> List[str]:\n        \"\"\"\n        Generate key insights from ML analysis\n        \n        Args:\n            total_anomalies: Number of anomalies detected\n            critical_anomalies: Number of critical anomalies\n            deteriorating_count: Number of deteriorating assets\n            top_factors: Top risk factors\n            \n        Returns:\n            List of insight strings\n        \"\"\"\n        insights = []\n        \n        if critical_anomalies > 0:\n            insights.append(f\"{critical_anomalies} assets show critical anomalous behavior requiring immediate review\")\n        \n        if deteriorating_count > 0:\n            insights.append(f\"{deteriorating_count} assets predicted to deteriorate in risk rating\")\n        \n        if total_anomalies > 0:\n            insights.append(f\"Anomaly detection identified {total_anomalies} assets with unusual patterns\")\n        \n        if top_factors:\n            insights.append(f\"Key risk drivers: {', '.join(top_factors)}\")\n        \n        if not insights:\n            insights.append(\"Portfolio shows stable risk patterns with no critical ML alerts\")\n        \n        return insights\n    \n    def _validate_ml_results(self, anomaly_results: List[Dict], \n                            risk_predictions: Dict, \n                            feature_importance: List[Dict],\n                            features_df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"\n        Validate ML results for quality and consistency\n        \n        Args:\n            anomaly_results: Anomaly detection results\n            risk_predictions: Risk prediction results\n            feature_importance: Feature importance scores\n            features_df: Feature DataFrame\n            \n        Returns:\n            Validation results with checks and warnings\n        \"\"\"\n        validation_checks = []\n        warnings = []\n        overall_status = 'PASS'\n        \n        # 1. Validate Anomaly Detection Results\n        anomaly_check = self._validate_anomaly_detection(anomaly_results)\n        validation_checks.append(anomaly_check)\n        if anomaly_check['status'] == 'WARNING':\n            warnings.extend(anomaly_check['issues'])\n        if anomaly_check['status'] == 'FAIL':\n            overall_status = 'FAIL'\n        \n        # 2. Validate Risk Prediction Results\n        prediction_check = self._validate_risk_predictions(risk_predictions)\n        validation_checks.append(prediction_check)\n        if prediction_check['status'] == 'WARNING':\n            warnings.extend(prediction_check['issues'])\n        if prediction_check['status'] == 'FAIL':\n            overall_status = 'FAIL'\n        \n        # 3. Validate Feature Quality\n        feature_check = self._validate_feature_quality(features_df)\n        validation_checks.append(feature_check)\n        if feature_check['status'] == 'WARNING':\n            warnings.extend(feature_check['issues'])\n        if feature_check['status'] == 'FAIL':\n            overall_status = 'FAIL'\n        \n        # 4. Validate Feature Importance\n        importance_check = self._validate_feature_importance(feature_importance)\n        validation_checks.append(importance_check)\n        if importance_check['status'] == 'WARNING':\n            warnings.extend(importance_check['issues'])\n        \n        # Set overall status based on checks\n        if overall_status != 'FAIL' and warnings:\n            overall_status = 'WARNING'\n        \n        return {\n            'overall_status': overall_status,\n            'validation_checks': validation_checks,\n            'warnings': warnings,\n            'total_checks': len(validation_checks),\n            'passed_checks': sum(1 for c in validation_checks if c['status'] == 'PASS'),\n            'validation_timestamp': datetime.now().isoformat()\n        }\n    \n    def _validate_anomaly_detection(self, anomaly_results: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Validate anomaly detection results\"\"\"\n        issues = []\n        \n        # Check anomaly scores are in valid range\n        for result in anomaly_results:\n            score = result['anomaly_score']\n            if not 0 <= score <= 100:\n                issues.append(f\"Invalid anomaly score {score} for {result['symbol']}\")\n        \n        # Validate severity classifications\n        for result in anomaly_results:\n            score = result['anomaly_score']\n            severity = result['severity']\n            \n            expected_severity = None\n            if score >= 80:\n                expected_severity = 'CRITICAL'\n            elif score >= 60:\n                expected_severity = 'HIGH'\n            elif score >= 40:\n                expected_severity = 'MEDIUM'\n            else:\n                expected_severity = 'LOW'\n            \n            if severity != expected_severity:\n                issues.append(f\"Severity mismatch for {result['symbol']}: score {score} has severity {severity}, expected {expected_severity}\")\n        \n        # Check anomaly rate is reasonable (expected ~15%)\n        anomaly_count = sum(1 for r in anomaly_results if r['is_anomaly'])\n        anomaly_rate = anomaly_count / len(anomaly_results) * 100 if anomaly_results else 0\n        \n        if anomaly_rate > 30:\n            issues.append(f\"High anomaly rate: {anomaly_rate:.1f}% (expected ~15%)\")\n        elif anomaly_rate < 5 and len(anomaly_results) >= 20:\n            issues.append(f\"Low anomaly rate: {anomaly_rate:.1f}% (expected ~15%)\")\n        \n        # Validate critical anomalies have high scores\n        critical_anomalies = [r for r in anomaly_results if r['severity'] == 'CRITICAL']\n        for result in critical_anomalies:\n            if result['anomaly_score'] < 80:\n                issues.append(f\"Critical anomaly {result['symbol']} has score below 80: {result['anomaly_score']}\")\n        \n        status = 'FAIL' if any('Invalid' in i for i in issues) else 'WARNING' if issues else 'PASS'\n        \n        return {\n            'check_name': 'Anomaly Detection Validation',\n            'status': status,\n            'issues': issues,\n            'metrics': {\n                'total_assets': len(anomaly_results),\n                'anomalies_detected': anomaly_count,\n                'anomaly_rate': round(anomaly_rate, 1),\n                'critical_count': len(critical_anomalies)\n            }\n        }\n    \n    def _validate_risk_predictions(self, risk_predictions: Dict) -> Dict[str, Any]:\n        \"\"\"Validate risk prediction results\"\"\"\n        issues = []\n        \n        if not risk_predictions.get('model_trained'):\n            return {\n                'check_name': 'Risk Prediction Validation',\n                'status': 'WARNING',\n                'issues': ['Risk prediction model not trained - insufficient data'],\n                'metrics': {}\n            }\n        \n        predictions = risk_predictions.get('predictions', [])\n        \n        # Validate confidence scores\n        for pred in predictions:\n            confidence = pred['confidence']\n            if not 0 <= confidence <= 100:\n                issues.append(f\"Invalid confidence score {confidence} for {pred['symbol']}\")\n        \n        # Check model accuracy is reasonable\n        test_accuracy = risk_predictions.get('test_accuracy', 0)\n        if test_accuracy < 50:\n            issues.append(f\"Low model accuracy: {test_accuracy}% (below 50%)\")\n        elif test_accuracy > 99:\n            issues.append(f\"Suspiciously high accuracy: {test_accuracy}% (possible overfitting)\")\n        \n        # Validate trend analysis consistency\n        for pred in predictions:\n            current = pred['current_rating']\n            predicted = pred['predicted_rating']\n            trend = pred['trend']\n            \n            rating_order = {'GREEN': 0, 'YELLOW': 1, 'RED': 2}\n            \n            if current == predicted and trend != 'STABLE':\n                issues.append(f\"Trend inconsistency for {pred['symbol']}: same rating but trend is {trend}\")\n            elif rating_order[predicted] > rating_order[current] and trend != 'DETERIORATING':\n                issues.append(f\"Trend inconsistency for {pred['symbol']}: worsening rating but trend is {trend}\")\n            elif rating_order[predicted] < rating_order[current] and trend != 'IMPROVING':\n                issues.append(f\"Trend inconsistency for {pred['symbol']}: improving rating but trend is {trend}\")\n        \n        # Check risk probabilities sum to ~100%\n        for pred in predictions:\n            probs = pred['risk_probabilities']\n            total_prob = sum(probs.values())\n            if not 99 <= total_prob <= 101:\n                issues.append(f\"Risk probabilities for {pred['symbol']} sum to {total_prob}% (should be ~100%)\")\n        \n        status = 'FAIL' if any('Invalid' in i for i in issues) else 'WARNING' if issues else 'PASS'\n        \n        return {\n            'check_name': 'Risk Prediction Validation',\n            'status': status,\n            'issues': issues,\n            'metrics': {\n                'model_accuracy': test_accuracy,\n                'predictions_made': len(predictions),\n                'rating_changes': sum(1 for p in predictions if p['rating_change'])\n            }\n        }\n    \n    def _validate_feature_quality(self, features_df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Validate feature data quality\"\"\"\n        issues = []\n        \n        # Check for NaN values\n        nan_counts = features_df[self.feature_columns].isna().sum()\n        for feature, count in nan_counts.items():\n            if count > 0:\n                issues.append(f\"Feature '{feature}' has {count} NaN values\")\n        \n        # Check for infinite values\n        for feature in self.feature_columns:\n            if np.isinf(features_df[feature]).any():\n                issues.append(f\"Feature '{feature}' contains infinite values\")\n        \n        # Check feature variance (low variance may indicate data quality issues)\n        for feature in self.feature_columns:\n            variance = features_df[feature].var()\n            if variance < 0.0001:\n                issues.append(f\"Feature '{feature}' has very low variance ({variance:.6f})\")\n        \n        status = 'FAIL' if any('NaN' in i or 'infinite' in i for i in issues) else 'WARNING' if issues else 'PASS'\n        \n        return {\n            'check_name': 'Feature Quality Validation',\n            'status': status,\n            'issues': issues,\n            'metrics': {\n                'features_checked': len(self.feature_columns),\n                'nan_features': int(nan_counts.sum()),\n                'total_samples': len(features_df)\n            }\n        }\n    \n    def _validate_feature_importance(self, feature_importance: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Validate feature importance results\"\"\"\n        issues = []\n        \n        if not feature_importance:\n            return {\n                'check_name': 'Feature Importance Validation',\n                'status': 'WARNING',\n                'issues': ['No feature importance calculated'],\n                'metrics': {}\n            }\n        \n        # Check importance values sum to ~100%\n        total_importance = sum(f['importance'] for f in feature_importance)\n        if not 99 <= total_importance <= 101:\n            issues.append(f\"Feature importances sum to {total_importance}% (should be ~100%)\")\n        \n        # Check for negative importance\n        for feature in feature_importance:\n            if feature['importance'] < 0:\n                issues.append(f\"Negative importance for {feature['feature']}: {feature['importance']}\")\n        \n        # Warn if one feature dominates (>60%)\n        top_importance = feature_importance[0]['importance'] if feature_importance else 0\n        if top_importance > 60:\n            issues.append(f\"Single feature dominates: {feature_importance[0]['feature']} ({top_importance}%)\")\n        \n        status = 'WARNING' if issues else 'PASS'\n        \n        return {\n            'check_name': 'Feature Importance Validation',\n            'status': status,\n            'issues': issues,\n            'metrics': {\n                'total_importance': round(total_importance, 1),\n                'top_feature': feature_importance[0]['feature'] if feature_importance else None,\n                'top_importance': round(top_importance, 1)\n            }\n        }\n","size_bytes":29260}},"version":2}